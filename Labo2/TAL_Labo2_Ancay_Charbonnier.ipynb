{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://heig-vd.ch/docs/default-source/doc-global-newsletter/2020-slim.svg\" alt=\"HEIG-VD Logo\" width=\"100\" align=\"right\" /> \n",
    "\n",
    "# Cours TAL - Laboratoire 2<br/>*POS taggers* pour le français dans spaCy et NLTK\n",
    "\n",
    "**Objectif**\n",
    "\n",
    "Comparer l'étiqueteur morphosyntaxique français prêt-à-l'emploi de spaCy avec deux étiqueteurs entraînés, l'un dans spaCy et l'autre dans NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation et test de spaCy\n",
    "\n",
    "La boîte à outils spaCy est une librairie Python *open source* pour le TAL, dédiée à un usage en production. Les documents suivants vous seront utiles :\n",
    "* comment [installer](https://spacy.io/usage) spaCy\n",
    "* comment [télécharger un modèle](https://spacy.io/usage/models) pour une langue donnée (on appelle ces modèles des *trained pipelines* car ils enchaînent plusieurs traitements)\n",
    "* comment faire les [premiers pas](https://spacy.io/usage/spacy-101) dans l'utilisation de spaCy\n",
    "\n",
    "Veuillez installer spaCy, puis la *pipeline* pour le français appelée `fr_core_news_sm`.  Si vous utilisez *conda*, installez spaCy dans l'environnement du cours TAL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_sm\") # charge la pipeline\n",
    "import tqdm # permet l'affichage d'une barre de progression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1a.** Une pipeline effectue un ensemble de traitements d'un texte en lui ajoutant des annotations.  Les traitements effectués par la pipeline `fr_core_news_sm` sont [documentés ici](https://spacy.io/models/fr#fr_core_news_sm).  La liste des traitements d'une pipeline figure dans son attribut `.pipe_names`.  On peut activer ou désactiver un traitement T avec, respectivement, les méthodes `.disable_pipe(T)` et `.enable_pipe(T)` appliquées à la pipeline.\n",
    "\n",
    "* Veuillez afficher les traitements disponibles dans la pipeline `fr_core_news_sm` chargée ci-dessus sous le nom de `nlp` .\n",
    "* Veuillez désactiver tous les traitements sauf `tok2vec` et `morphologizer` (on fait cela pour accélerer le traitement).\n",
    "* Vérifiez que la désactivation a bien fonctionné en affichant les traitements activés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avant -> ['tok2vec', 'morphologizer', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "Après :) -> ['tok2vec', 'morphologizer']\n"
     ]
    }
   ],
   "source": [
    "# Veuillez écrire votre code ici.\n",
    "print('Avant ->', nlp.pipe_names)\n",
    "for name in nlp.pipe_names:\n",
    "    nlp.disable_pipe(name)\n",
    "\n",
    "nlp.enable_pipe('tok2vec')\n",
    "nlp.enable_pipe('morphologizer')\n",
    "print('Après :) ->', nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.fr.examples import sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1b.** L'objet `sentences` chargé ci-dessus contient une liste de phrases en français. \n",
    "\n",
    "* Veuillez afficher les deux premières phrases de `sentences`.\n",
    "* Veuillez analyser chacune de ces deux phrases avec la pipeline `nlp` puis afficher chaque token et son POS tag.\n",
    "    * indication : aidez-vous de la [documentation](https://spacy.io/models/fr#fr_core_news_sm) de `fr_core_news_sm`\n",
    "    * consigne d'affichage : indiquer le tag entre crochets après chaque token, comme ceci : Les \\[DET\\] robots \\[NOUN\\] ...\n",
    "    * note : la documentation détaillée du POS tagging dans spaCy est [disponible ici](https://spacy.io/usage/linguistic-features)\n",
    "* Veuillez commenter la tokenisation et les POS tags observés : vous semblent-ils corrects pour les deux phrases ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_analyzed_sentence(sentence):\n",
    "    analyzed_sentence = nlp(sentence)\n",
    "    for tok in analyzed_sentence:\n",
    "        print('\\t-', tok.text, tok.pos_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple cherche à acheter une start-up anglaise pour 1 milliard de dollars', \"Les voitures autonomes déplacent la responsabilité de l'assurance vers les constructeurs\"]\n",
      "Phrase N° 1 :\n",
      "\n",
      "\t- Apple NOUN\n",
      "\t- cherche NOUN\n",
      "\t- à ADP\n",
      "\t- acheter VERB\n",
      "\t- une DET\n",
      "\t- start NOUN\n",
      "\t- - NOUN\n",
      "\t- up ADJ\n",
      "\t- anglaise NOUN\n",
      "\t- pour ADP\n",
      "\t- 1 NUM\n",
      "\t- milliard NOUN\n",
      "\t- de ADP\n",
      "\t- dollars NOUN\n",
      "Phrase N° 2 :\n",
      "\n",
      "\t- Les DET\n",
      "\t- voitures NOUN\n",
      "\t- autonomes ADJ\n",
      "\t- déplacent ADV\n",
      "\t- la DET\n",
      "\t- responsabilité NOUN\n",
      "\t- de ADP\n",
      "\t- l' DET\n",
      "\t- assurance NOUN\n",
      "\t- vers ADP\n",
      "\t- les DET\n",
      "\t- constructeurs NOUN\n"
     ]
    }
   ],
   "source": [
    "# Veuillez écrire votre code et votre commentaire ici.\n",
    "print(sentences[0:2])\n",
    "for i in range(2):\n",
    "    print('Phrase N°',i+1,':\\n')\n",
    "    print_analyzed_sentence(sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Commentaires :*\n",
    "\n",
    "La tokenisation est correcte dans l'ensemble mais on aurait préféré que le mot \"start-up\" de la première phrase ne soit pas divisés en 3 tokens différents. La tokenisation de la deuxième phrase semble correcte.\n",
    "\n",
    "Concernant le tagging, il y a plusieurs problèmes :\n",
    "- Phrase N°1 :\n",
    "    - 'cherche' devrait être un verbe, pas un nom\n",
    "    - étant donné que le mot 'start-up' a été divisé en plusieurs tokens, le tagging n'a plus trop de sens\n",
    "    - 'anglaise' devrait être un adjectif\n",
    "- Phrase N°2 :\n",
    "  - 'déplacent' est un verbe conjugué, pas un adverbe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prise en main des données\n",
    "\n",
    "Les données sont fournies dans un format tabulaire dans l'archive `UD_French-GSD.zip` sur Cyberlearn.  Elles sont basées sur les données fournies par le projet [Universal Dependencies](https://github.com/UniversalDependencies/UD_French-GSD).  Leur format, appelé CoNLL-U, est [documenté ici](https://universaldependencies.org/format.html).  Veuillez placer les trois fichiers contenus dans l'archive dans un sous-dossier de ce notebook nommé `spacy_data`.\n",
    "\n",
    "Les trois fichiers contiennent des phrases en français annotées avec les POS tags :\n",
    "* le fichier `fr-ud-train.conllu` est destiné à l'entraînement\n",
    "* le fichier `fr-ud-dev.conllu` est destiné aux tests préliminaires et aux réglages des paramètres\n",
    "* le fichier `fr-ud-test.conllu` est destiné à l'évaluation finale.\n",
    "\n",
    "**2a.** En inspectant les fichiers avec un éditeur texte, veuillez déterminer dans quelle colonne se trouvent les *tokens* des textes originaux, et dans quelle colonne se trouvent leurs étiquettes morpho-syntaxiques correctes (*POS tags*).  Que contient la troisième colonne ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire vos réponses dans cette cellule.\n",
    "# Colonne 0 : N° du token\n",
    "# Colonne 1 : token\n",
    "# Colonne 2 : lemme du token\n",
    "# Colonne 3 : Universal POS Tag (selon la doc du format CoNLL-U)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2b.** Veuillez convertir les trois fichiers de données en des fichiers binaires utilisables par spaCy, en utilisant la [commande 'convert' fournie par spaCy](https://spacy.io/api/cli#convert).  La commande est donnée ci-dessous, le premier dossier `./input_data` contient les 3 fichiers `.conllu` et le dossier `./spacy-data` contiendra les 3 résultats.\n",
    "\n",
    "* Veuillez exécuter la commande de conversion.\n",
    "* Combien de phrases environ (à 10 phrases près) contient chaque fichier (*train*, *dev*, *test*) ?  Observez la commande et son résultat pour répondre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy convert ./data ./spacy_data --converter conllu  --n-sents 10 --lang fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez indiquer les nombres de phrases ici.\n",
    "# dev -> 148 * 10 phrases = 1480\n",
    "# test -> 42 * 10 phrases = 420\n",
    "# train -> 1456 * 10 phrases = 14560\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2c**. Les données des fichiers convertis peuvent être chargées dans un objet de type `DocBin`.  Dans notre cas, un tel objet contient un ensemble de documents, chacun contenant 10 phrases.  Chaque document est un objet de type `Doc`.  Le code donné ci-dessous vous permet de charger les données de test et vous montre comment les afficher.\n",
    "\n",
    "* Veuillez stocker la première phrase des données de test dans une variable nommée `premiere_phrase_test`.\n",
    "* Veuillez afficher cette phrase, ainsi que son type dans spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "from spacy.tokens import Doc\n",
    "test_data = DocBin().from_disk(\"./spacy_data/fr-ud-test.spacy\")\n",
    "# Exemple d'utilisation (afficher toutes les phrases)\n",
    "# for doc in test_data.get_docs(nlp.vocab): \n",
    "#     for sent in doc.sents:\n",
    "#         print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je sens qu'entre ça et les films de médecins et scientifiques fous que nous avons déjà vus, nous pourrions emprunter un autre chemin pour l'origine.\n"
     ]
    }
   ],
   "source": [
    "# Veuillez écrire votre code ici.\n",
    "docs = test_data.get_docs(nlp.vocab)\n",
    "first_document = next(docs)\n",
    "sentences_first_document = first_document.sents\n",
    "premiere_phrase_test = next(sentences_first_document)\n",
    "print(premiere_phrase_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Évaluation du POS tagger français de la pipeline `fr_core_news_sm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3a.** Veuillez effectuer le *POS tagging* avec spaCy de la `premiere_phrase_test` et afficher les résultats dans le format demandé au (1b).  Indication : convertissez la `premiere_phrase_test` dans un objet de type `Doc` en lui appliquant la méthode `.as_doc()`.  Cet objet peut être ensuite traité par la pipeline `nlp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Je PRON\n",
      "\t- sens VERB\n",
      "\t- qu' SCONJ\n",
      "\t- entre ADP\n",
      "\t- ça PRON\n",
      "\t- et CCONJ\n",
      "\t- les DET\n",
      "\t- films NOUN\n",
      "\t- de ADP\n",
      "\t- médecins NOUN\n",
      "\t- et CCONJ\n",
      "\t- scientifiques NOUN\n",
      "\t- fous PRON\n",
      "\t- que PRON\n",
      "\t- nous PRON\n",
      "\t- avons AUX\n",
      "\t- déjà ADV\n",
      "\t- vus VERB\n",
      "\t- , PUNCT\n",
      "\t- nous PRON\n",
      "\t- pourrions VERB\n",
      "\t- emprunter VERB\n",
      "\t- un DET\n",
      "\t- autre ADJ\n",
      "\t- chemin NOUN\n",
      "\t- pour ADP\n",
      "\t- l' DET\n",
      "\t- origine NOUN\n",
      "\t- . PUNCT\n"
     ]
    }
   ],
   "source": [
    "# Veuillez écrire votre code ici.\n",
    "\n",
    "doc_premiere_phrase_test = premiere_phrase_test.as_doc()\n",
    "\n",
    "print_analyzed_sentence(doc_premiere_phrase_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3b.** Veuillez afficher les tags corrects de `premiere_phrase_test`, puis comparez-les visuellement les tags trouvés automatiquement au (3a).  Quelles différences trouvez-vous ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparaison complète : \n",
      "Je  -----> Prédit :  PRON | Selon fichier :  PRON\n",
      "sens  -----> Prédit :  VERB | Selon fichier :  VERB\n",
      "qu'  -----> Prédit :  SCONJ | Selon fichier :  SCONJ\n",
      "entre  -----> Prédit :  ADP | Selon fichier :  ADP\n",
      "ça  -----> Prédit :  PRON | Selon fichier :  PRON\n",
      "et  -----> Prédit :  CCONJ | Selon fichier :  CCONJ\n",
      "les  -----> Prédit :  DET | Selon fichier :  DET\n",
      "films  -----> Prédit :  NOUN | Selon fichier :  NOUN\n",
      "de  -----> Prédit :  ADP | Selon fichier :  ADP\n",
      "médecins  -----> Prédit :  NOUN | Selon fichier :  NOUN\n",
      "et  -----> Prédit :  CCONJ | Selon fichier :  CCONJ\n",
      "scientifiques  -----> Prédit :  NOUN | Selon fichier :  NOUN\n",
      "fous  -----> Prédit :  PRON | Selon fichier :  ADJ\n",
      "que  -----> Prédit :  PRON | Selon fichier :  PRON\n",
      "nous  -----> Prédit :  PRON | Selon fichier :  PRON\n",
      "avons  -----> Prédit :  AUX | Selon fichier :  AUX\n",
      "déjà  -----> Prédit :  ADV | Selon fichier :  ADV\n",
      "vus  -----> Prédit :  VERB | Selon fichier :  VERB\n",
      ",  -----> Prédit :  PUNCT | Selon fichier :  PUNCT\n",
      "nous  -----> Prédit :  PRON | Selon fichier :  PRON\n",
      "pourrions  -----> Prédit :  VERB | Selon fichier :  VERB\n",
      "emprunter  -----> Prédit :  VERB | Selon fichier :  VERB\n",
      "un  -----> Prédit :  DET | Selon fichier :  DET\n",
      "autre  -----> Prédit :  ADJ | Selon fichier :  ADJ\n",
      "chemin  -----> Prédit :  NOUN | Selon fichier :  NOUN\n",
      "pour  -----> Prédit :  ADP | Selon fichier :  ADP\n",
      "l'  -----> Prédit :  DET | Selon fichier :  DET\n",
      "origine  -----> Prédit :  NOUN | Selon fichier :  NOUN\n",
      ".  -----> Prédit :  PUNCT | Selon fichier :  PUNCT\n",
      "Différences uniquement : \n",
      "fous  -----> Prédit :  PRON | Selon fichier :  ADJ\n"
     ]
    }
   ],
   "source": [
    "# Veuillez écrire votre réponse ici.\n",
    "comparison = []\n",
    "\n",
    "for i, token in enumerate(doc_premiere_phrase_test):\n",
    "    element = []\n",
    "    element.append(token.text)\n",
    "    element.append(token.pos_) # .pos_ -> prédiction\n",
    "    element.append(token.tag_) # .tag_ -> valeur de référence (provenant du fichier)\n",
    "    comparison.append(element)\n",
    "\n",
    "# Comparaison complète\n",
    "print(\"Comparaison complète : \")\n",
    "for elem in comparison:\n",
    "    print(elem[0], ' -----> Prédit : ', elem[1], '| Selon fichier : ', elem[2])\n",
    "\n",
    "print(\"Différences uniquement : \")\n",
    "for elem in comparison:\n",
    "    if(elem[1] != elem[2]):\n",
    "        print(elem[0], ' -----> Prédit : ', elem[1], '| Selon fichier : ', elem[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Réponse :* Le seul tag différent pour la première phrase est pour le mot \"fous\". C'est un adjectif, mais nlp l'a classifié comme un pronom (choix un peu étrange...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.scorer import Scorer\n",
    "from spacy.training import Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = Scorer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3c.** Au lieu de compter manuellement combien de tags sont différents entre la référence et le résultat de la pipeline `nlp`, vous allez utiliser la classe `Scorer` de spaCy.  Une instance de cette classe permet de calculer les scores d'une liste d'objets de type `Exemple`, en fonction des annotations disponibles dans les objets.  Un objet de type `Exemple` contient deux objets de type `Doc`, l'un avec les annotations correctes et l'autre avec les annotations produites par une pipeline.  La [documentation de la méthode](https://spacy.io/api/scorer#score) `Scorer.score(..)` vous sera utile. \n",
    "\n",
    "* Veuillez calculer la justesse (*accuracy*) du *POS tagging* de `premiere_phrase_test`. \n",
    "* Veuillez justifier la valeur du score obtenu en utilisant votre réponse du (3b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 1.0\n",
      "Vérification : 0.9655172413793104\n"
     ]
    }
   ],
   "source": [
    "# Veuillez écrire votre code ici.\n",
    "docPos = [tok.pos_ for tok in doc_premiere_phrase_test]\n",
    "\n",
    "example = Example(nlp(premiere_phrase_test.as_doc()), premiere_phrase_test.as_doc())\n",
    "\n",
    "print('Accuracy :', scorer.score([example])['tag_acc'])\n",
    "\n",
    "print('Vérification :', 1 - 1/len(doc_premiere_phrase_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3d.** Veuillez calculer la précision du *POS tagging* de la pipeline `nlp` sur toutes les données de test présentes dans `test_data`.  Comment se compare le score obtenu avec celui mentionné [dans la documentation](https://spacy.io/models/fr#fr_core_news_sm) du modèle `fr_core_news_sm` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9172654690618762\n"
     ]
    }
   ],
   "source": [
    "# Veuillez écrire votre code ici, suivi de votre réponse à la question.\n",
    "\n",
    "scorer = Scorer()\n",
    "\n",
    "examples = []\n",
    "for doc in test_data.get_docs(nlp.vocab):\n",
    "    for sent in doc.sents:\n",
    "        example = Example(nlp(sent.as_doc()), sent.as_doc())\n",
    "        examples.append(example)\n",
    "print('Accuracy :', scorer.score(examples)['pos_acc'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Réponse :* Nous n'obtenons pas exactement le même score (0.91 au lieu de 0.96 dans la doc). C'est un petite différence donc on peut en conclure que l'ensemble de test est plus représentatif."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entraîner puis évaluer un nouveau POS tagger français dans spaCy\n",
    "\n",
    "Le but de cette partie est d'entraîner une pipeline spaCy pour le français sur les données de `fr-ud-train.conllu`, puis de comparer le modèle obtenu avec le modèle prêt-à-l'emploi testé au point précédent.  Les [instructions d'entraînement](https://spacy.io/usage/training#quickstart) de spaCy vous montrent comment entraîner une pipeline avec un POS tagger.\n",
    "\n",
    "**4a.** Paramétrage de l'entraînement :\n",
    "* générez un fichier de départ grâce à [l'interface web](https://spacy.io/usage/training#quickstart), en indiquant que vous voulez seulement un POS tagger dans la pipeline ;\n",
    "* sauvegardez le code généré par spaCy dans un fichier local `base_config.cfg` ;\n",
    "* générez un fichier `config.cfg` sur votre ordinateur en exécutant la ligne de commande suivante. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config base_config.cfg config.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, veuillez effectuer l'entraînement avec la ligne de commande suivante.  Faites plusieurs essais, d'abord avec un petit nombre d'époques, pour estimer le temps nécessaire et observer les messages affichés.  Puis augmentez progressivement le nombre d'époques.  Quel est le critère qui vous permet de décider que vous avez un nombre suffisant d'époques ?  Dans quel dossier se trouve le meilleur modèle ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Created output directory: myPOStagger1\u001b[0m\n",
      "\u001b[38;5;4mℹ Saving to output directory: myPOStagger1\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'tagger']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS TAGGER  TAG_ACC  SCORE \n",
      "---  ------  ------------  -----------  -------  ------\n",
      "  0       0          0.00       211.77    36.34    0.36\n",
      "  0     200        315.96     10403.17    90.36    0.90\n",
      "  0     400        287.45      4496.17    91.62    0.92\n",
      "  0     600        223.10      3449.18    92.17    0.92\n",
      "  0     800        216.83      3458.35    92.55    0.93\n",
      "  0    1000        192.05      3030.81    92.62    0.93\n",
      "  0    1200        185.09      2922.23    92.96    0.93\n",
      "  0    1400        172.84      2749.04    92.98    0.93\n",
      "  1    1600        147.28      2230.07    93.11    0.93\n",
      "  1    1800        138.32      2029.00    93.12    0.93\n",
      "  1    2000        155.07      2294.50    93.18    0.93\n",
      "  1    2200        144.89      2145.05    93.30    0.93\n",
      "  1    2400        145.82      2132.37    93.26    0.93\n",
      "  1    2600        149.43      2169.59    93.28    0.93\n",
      "  1    2800        152.79      2200.83    93.41    0.93\n",
      "  2    3000        132.99      1853.46    93.33    0.93\n",
      "  2    3200        120.85      1584.79    93.23    0.93\n",
      "  2    3400        134.05      1742.55    93.51    0.94\n",
      "  2    3600        128.47      1632.46    93.52    0.94\n",
      "  2    3800        126.12      1586.33    93.43    0.93\n",
      "  2    4000        132.55      1694.30    93.49    0.93\n",
      "  2    4200        140.97      1755.24    93.48    0.93\n",
      "  3    4400        139.11      1687.01    93.55    0.94\n",
      "  3    4600         99.66      1161.66    93.52    0.94\n",
      "  3    4800        112.63      1283.74    93.53    0.94\n",
      "  3    5000        124.49      1427.55    93.52    0.94\n",
      "  3    5200        120.60      1366.04    93.57    0.94\n",
      "  3    5400        125.39      1433.67    93.48    0.93\n",
      "  3    5600        123.23      1393.55    93.62    0.94\n",
      "  3    5800        129.08      1439.31    93.61    0.94\n",
      "  4    6000        108.59      1163.19    93.65    0.94\n",
      "  4    6200        103.67      1080.86    93.52    0.94\n",
      "  4    6400        108.60      1118.58    93.55    0.94\n",
      "  4    6600        118.45      1228.04    93.57    0.94\n",
      "  4    6800        119.94      1242.56    93.55    0.94\n",
      "  4    7000        122.15      1249.80    93.58    0.94\n",
      "  4    7200        111.17      1118.86    93.62    0.94\n",
      "  5    7400        101.14      1005.76    93.61    0.94\n",
      "  5    7600         99.32       949.92    93.52    0.94\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "myPOStagger1\\model-last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-13 14:39:49,083] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev']\n",
      "[2025-03-13 14:39:54,855] [INFO] Set up nlp object from config\n",
      "[2025-03-13 14:39:54,873] [DEBUG] Loading corpus from path: spacy_data\\fr-ud-dev.spacy\n",
      "[2025-03-13 14:39:54,878] [DEBUG] Loading corpus from path: spacy_data\\fr-ud-train.spacy\n",
      "[2025-03-13 14:39:54,878] [INFO] Pipeline: ['tok2vec', 'tagger']\n",
      "[2025-03-13 14:39:54,881] [INFO] Created vocabulary\n",
      "[2025-03-13 14:39:54,881] [INFO] Finished initializing nlp object\n",
      "[2025-03-13 14:40:17,342] [INFO] Initialized pipeline components: ['tok2vec', 'tagger']\n",
      "[2025-03-13 14:40:17,359] [DEBUG] Loading corpus from path: spacy_data\\fr-ud-dev.spacy\n",
      "[2025-03-13 14:40:17,361] [DEBUG] Loading corpus from path: spacy_data\\fr-ud-train.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train config.cfg \\\n",
    "  --output ./myPOStagger1 \\\n",
    "  --paths.train ./spacy_data/fr-ud-train.spacy \\\n",
    "  --paths.dev ./spacy_data/fr-ud-dev.spacy \\\n",
    "  --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Réponse :*\n",
    "\n",
    "Le modèle s'est entraîné pendant 5 epochs. Dans l'interface permettant de générer le fichier de config, nous avons sélectionné \"efficiency\" comme optimisation. C'est donc sûrement la métrique (`TAG_ACC`) qui est utilisée comme critère d'arrêt. Le meilleur modèle se trouve dans `myPOStagger1/model-best/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4b.**  Veuillez charger le meilleur modèle (pipeline) dans la variable `nlp2` et afficher la *POS tagging accuracy* sur le corpus de test.  Le composant de la pipeline étant un *POS tagger*, vous devrez évaluer la propriété *tag_acc*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire votre code ici.\n",
    "nlp2 = spacy.load('myPOStagger1/model-best/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9586327345309381\n"
     ]
    }
   ],
   "source": [
    "for doc in test_data.get_docs(nlp2.vocab):\n",
    "    for sent in doc.sents:\n",
    "        example = Example(nlp2(sent.as_doc()), sent.as_doc())\n",
    "        examples.append(example)\n",
    "print('Accuracy :', scorer.score(examples)['pos_acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Commentaire :*\n",
    "\n",
    "L'accuracy est légèrement meilleure : ~0.96 au lieu de ~0.92."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entraîner puis évaluer un POS tagger pour le français dans NLTK\n",
    "\n",
    "Le but de cette partie est d'utiliser le POS tagger appelé *Averaged Perceptron* fourni par NLTK, en l'entraînant pour le français sur les mêmes données que ci-dessus, importées cette fois-ci avec NLTK.  Pour une introduction au POS tagging avec NLTK, voir le [Chapitre 5.1 du livre NLTK](http://www.nltk.org/book/ch05.html).\n",
    "\n",
    "Remarques :\n",
    "* pour l'anglais, des taggers pré-entraînés sont disponibles dans NLTK ;\n",
    "* pour appliquer un tagger existant, on écrit `nltk.pos_tag(sentence)` où `sentence` est une liste de tokens et on obtient des paires (token, TAG) ;\n",
    "* l'implémentation de *Averaged Perceptron* a été faite par [Mathew Honnibal de Explosion.AI](https://explosion.ai/blog/part-of-speech-pos-tagger-in-python), la société qui a créé spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5a.** Veuillez charger les données d'entraînement et celles de test grâce à la classe `ConllCorpusReader` de NLTK.  [La documentation de cette classe](https://www.nltk.org/api/nltk.corpus.reader.conll.html#nltk.corpus.reader.conll.ConllCorpusReader) vous montrera comment indiquer les colonnes qui contiennent les tokens ('words') et les tags corrects ('pos').  Une fois les données chargées dans une variable, vous pouvez accéder aux phrases et aux tags avec la méthode `.tagged_sents()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.conll import ConllCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire votre code ici.\n",
    "\n",
    "corpus_train = ConllCorpusReader(root='./data', fileids='fr-ud-train.conllu', columntypes=['ignore', 'words', 'ignore', 'pos'], encoding='utf8')\n",
    "corpus_test = ConllCorpusReader(root='./data', fileids='fr-ud-test.conllu', columntypes=['ignore', 'words', 'ignore', 'pos'], encoding='utf8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5b.** Pour entraîner un POS tagger du type Averaged Perceptron, vous utiliserez le sous-module `nltk.tag.perceptron` du [module NLTK contenant les taggers](http://www.nltk.org/api/nltk.tag.html).  Les fonctions d'entraînement et de test sont documentées dans ce module.  Après l'entraînement, le réseau de neurones est enregistré dans un fichier `.pickle`, qui est écrasé à chaque entraînement si vous n'en faites pas une copie.  On peut également lire un fichier `.pickle` dans un tagger.\n",
    "\n",
    "Veuillez écrire le code pour entraîner le POS tagger sur les données d'entraînement.  Comme au (4), pensez augmenter graduellement le nombre d'époques (appelées 'itérations' dans NLTK).\n",
    "\n",
    "Combien de temps prend l'entraînement ?  Quelle est la taille du fichier enregistré ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import nltk\n",
    "# nltk.download('averaged_perceptron_tagger') # à exécuter la première fois\n",
    "from nltk.tag.perceptron import PerceptronTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptagger = PerceptronTagger(load=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Les', 'DET'), ('commotions', 'NOUN'), ('cérébrales', 'ADJ'), ('sont', 'AUX'), ('devenu', 'VERB'), ('si', 'ADV'), ('courantes', 'ADJ'), ('dans', 'ADP'), ('ce', 'DET'), ('sport', 'NOUN'), (\"qu'\", 'SCONJ'), ('on', 'PRON'), ('les', 'PRON'), ('considére', 'VERB'), ('presque', 'ADV'), ('comme', 'ADP'), ('la', 'DET'), ('routine', 'NOUN'), ('.', 'PUNCT')]\n",
      "[(\"L'\", 'DET'), ('œuvre', 'NOUN'), ('est', 'AUX'), ('située', 'VERB'), ('dans', 'ADP'), ('la', 'DET'), ('galerie', 'NOUN'), ('des', '_'), ('de', 'ADP'), ('les', 'DET'), ('batailles', 'NOUN'), (',', 'PUNCT'), ('dans', 'ADP'), ('le', 'DET'), ('château', 'NOUN'), ('de', 'ADP'), ('Versailles', 'PROPN'), ('.', 'PUNCT')]\n",
      "[('Le', 'DET'), ('comportement', 'NOUN'), ('de', 'ADP'), ('la', 'DET'), ('Turquie', 'PROPN'), ('vis-à-vis', 'NOUN'), ('du', '_'), ('de', 'ADP'), ('le', 'DET'), ('problème', 'NOUN'), ('palestinien', 'ADJ'), ('a', 'AUX'), ('fait', 'VERB'), (\"qu'\", 'SCONJ'), ('elle', 'PRON'), (\"n'\", 'ADV'), ('est', 'VERB'), ('plus', 'ADV'), ('en', 'ADP'), ('odeur', 'NOUN'), ('de', 'ADP'), ('sainteté', 'NOUN'), ('auprès', 'ADP'), ('de', 'ADP'), ('la', 'DET'), ('communauté', 'NOUN'), ('juive', 'ADJ'), ('en', 'ADP'), ('générale', 'ADJ'), (',', 'PUNCT'), ('et', 'CCONJ'), ('américaine', 'NOUN'), ('en', 'ADP'), ('particulier', 'ADJ'), ('.', 'PUNCT')]\n",
      "[('Toutefois', 'ADV'), (',', 'PUNCT'), ('les', 'DET'), ('filles', 'NOUN'), ('adorent', 'VERB'), ('les', 'DET'), ('desserts', 'NOUN'), ('.', 'PUNCT')]\n",
      "[('Ismene', 'PROPN'), ('entre', 'VERB'), ('et', 'CCONJ'), ('annonce', 'VERB'), ('que', 'SCONJ'), (\"c'\", 'PRON'), ('est', 'AUX'), ('Farnace', 'PROPN'), ('qui', 'PRON'), ('a', 'AUX'), ('mis', 'VERB'), ('le', 'DET'), ('feu', 'NOUN'), ('à', 'ADP'), ('la', 'DET'), ('flotte', 'NOUN'), ('romaine', 'ADJ'), ('.', 'PUNCT')]\n",
      "[('je', 'PRON'), ('reviendrais', 'VERB'), ('avec', 'ADP'), ('plaisir', 'NOUN'), ('!', 'PUNCT')]\n",
      "[('Les', 'DET'), ('forfaits', 'NOUN'), ('comprennent', 'VERB'), ('le', 'DET'), ('transport', 'NOUN'), ('en', 'ADP'), ('car', 'NOUN'), ('Grand', 'PROPN'), ('Tourisme', 'PROPN'), ('des', '_'), ('de', 'ADP'), ('les', 'DET'), ('différents', 'ADJ'), ('lieux', 'NOUN'), ('de', 'ADP'), ('départs', 'NOUN'), ('proposés', 'VERB'), ('autour', 'ADP'), ('de', 'ADP'), ('Lyon', 'PROPN'), (',', 'PUNCT'), ('le', 'DET'), ('forfait', 'NOUN'), ('de', 'ADP'), ('ski', 'NOUN'), ('sur', 'ADP'), ('le', 'DET'), ('domaine', 'NOUN'), ('cité', 'VERB'), (',', 'PUNCT'), ('un', 'DET'), ('en-cas', 'NOUN'), ('petit-déjeuner', 'NOUN'), ('servi', 'VERB'), ('à', 'ADP'), ('votre', 'DET'), ('arrivée', 'NOUN'), ('en', 'ADP'), ('station', 'NOUN'), ('et', 'CCONJ'), ('le', 'DET'), ('service', 'NOUN'), (\"d'\", 'ADP'), ('un', 'DET'), ('accompagnateur', 'NOUN'), ('salarié', 'ADJ'), ('assurant', 'VERB'), ('la', 'DET'), ('logistique', 'NOUN'), ('des', '_'), ('de', 'ADP'), ('les', 'DET'), ('départs', 'NOUN'), ('et', 'CCONJ'), ('des', '_'), ('de', 'ADP'), ('les', 'DET'), ('retours', 'NOUN'), ('.', 'PUNCT')]\n",
      "[('Il', 'PRON'), ('prévient', 'VERB'), ('que', 'SCONJ'), ('de', 'DET'), ('telles', 'ADJ'), ('agressions', 'NOUN'), ('sont', 'AUX'), ('\"', 'PUNCT'), ('susceptibles', 'ADJ'), (\"d'\", 'ADP'), ('entraîner', 'VERB'), ('des', 'DET'), ('poursuites', 'NOUN'), ('judiciaires', 'ADJ'), ('\"', 'PUNCT'), ('.', 'PUNCT')]\n",
      "[('Ils', 'PRON'), ('tiraient', 'VERB'), ('à', 'ADP'), ('balles', 'NOUN'), ('réelles', 'ADJ'), ('sur', 'ADP'), ('la', 'DET'), ('foule', 'NOUN'), ('.', 'PUNCT')]\n",
      "[('Le', 'DET'), ('château', 'NOUN'), ('est', 'AUX'), ('ensuite', 'ADV'), ('vendu', 'VERB'), ('plusieurs', 'DET'), ('fois', 'NOUN'), (';', 'PUNCT')]\n",
      "[('En', 'ADP'), ('effet', 'NOUN'), (',', 'PUNCT'), ('la', 'DET'), ('biréfringence', 'NOUN'), (\"n'\", 'ADV'), ('étant', 'AUX'), ('pas', 'ADV'), ('visible', 'ADJ'), (',', 'PUNCT'), ('elle', 'PRON'), (\"n'\", 'ADV'), ('est', 'AUX'), ('pas', 'ADV'), ('prépondérante', 'ADJ'), ('sur', 'ADP'), ('le', 'DET'), ('pouvoir', 'NOUN'), ('rotatoire', 'ADJ'), ('.', 'PUNCT')]\n",
      "[('Le', 'DET'), ('point', 'NOUN'), ('final', 'ADJ'), ('de', 'ADP'), ('la', 'DET'), ('plate-forme', 'NOUN'), ('de', 'ADP'), ('distribution', 'NOUN'), ('serait', 'AUX'), ('basé', 'VERB'), ('en', 'ADP'), ('Autriche', 'PROPN'), ('à', 'ADP'), ('Baumgarten', 'PROPN'), ('pour', 'ADP'), ('rejoindre', 'VERB'), ('ensuite', 'ADV'), ('le', 'DET'), ('réseau', 'NOUN'), (\"d'\", 'ADP'), ('Europe', 'PROPN'), ('centrale', 'ADJ'), ('et', 'CCONJ'), ('de', 'ADP'), (\"l'\", 'DET'), ('ouest', 'NOUN'), ('.', 'PUNCT')]\n",
      "[(\"L'\", 'DET'), ('information', 'NOUN'), ('génétique', 'ADJ'), ('est', 'AUX'), ('codée', 'VERB'), ('sous', 'ADP'), ('forme', 'NOUN'), (\"d'\", 'ADP'), ('ADN', 'NOUN'), ('.', 'PUNCT')]\n",
      "[('Motivé', 'VERB'), ('par', 'ADP'), ('la', 'DET'), ('charité', 'NOUN'), ('chrétienne', 'ADJ'), (',', 'PUNCT'), ('la', 'DET'), ('communauté', 'NOUN'), ('a', 'AUX'), ('changé', 'VERB'), ('un', 'DET'), ('vieil', 'ADJ'), ('immeuble', 'NOUN'), ('abanbonné', 'VERB'), ('où', 'PRON'), ('se', 'PRON'), ('pratiquaient', 'VERB'), (\"d'\", 'ADP'), ('ailleurs', 'ADV'), ('de', 'DET'), ('nombreux', 'ADJ'), ('échanges', 'NOUN'), ('de', 'ADP'), ('drogues', 'NOUN'), ('en', 'ADP'), ('ce', 'DET'), ('logement', 'NOUN'), ('sûr', 'ADJ'), (',', 'PUNCT'), ('propre', 'ADJ'), ('et', 'CCONJ'), ('abordable', 'ADJ'), (\"qu'\", 'PRON'), ('il', 'PRON'), ('est', 'AUX'), (\"aujourd'hui\", 'ADV'), ('.', 'PUNCT')]\n",
      "[('Il', 'PRON'), ('exploitait', 'VERB'), ('un', 'DET'), ('site', 'NOUN'), ('de', 'ADP'), ('haute', 'ADJ'), ('valeur', 'NOUN'), ('militaire', 'ADJ'), ('et', 'CCONJ'), ('commerciale', 'ADJ'), ('comprenant', 'VERB'), ('un', 'DET'), ('surplomb', 'NOUN'), ('rocheux', 'ADJ'), ('dominant', 'VERB'), ('en', 'ADP'), ('à-pic', 'NOUN'), ('la', 'DET'), ('rivière', 'NOUN'), ('Oust', 'PROPN'), ('.', 'PUNCT')]\n",
      "[('Plus', 'ADV'), ('tard', 'ADV'), ('dans', 'ADP'), ('la', 'DET'), ('saison', 'NOUN'), (',', 'PUNCT'), ('ils', 'PRON'), ('consomment', 'VERB'), ('beaucoup', 'ADV'), ('de', 'ADP'), ('baies', 'NOUN'), ('et', 'CCONJ'), ('autres', 'ADJ'), ('petits', 'ADJ'), ('fruits', 'NOUN'), ('.', 'PUNCT')]\n",
      "[('Ils', 'PRON'), ('deviennent', 'VERB'), ('alors', 'ADV'), ('les', 'DET'), ('Paladins', 'PROPN'), (',', 'PUNCT'), ('chevaliers', 'NOUN'), ('du', '_'), ('de', 'ADP'), ('le', 'DET'), ('bien', 'NOUN'), ('chargés', 'VERB'), ('de', 'ADP'), ('défendre', 'VERB'), ('élèves', 'NOUN'), ('et', 'CCONJ'), ('professeurs', 'NOUN'), ('.', 'PUNCT')]\n",
      "[('Le', 'DET'), ('chevalier', 'NOUN'), ('lui', 'PRON'), ('propose', 'VERB'), ('une', 'DET'), ('partie', 'NOUN'), (\"d'\", 'ADP'), ('échecs', 'NOUN'), (',', 'PUNCT'), ('espérant', 'VERB'), ('retarder', 'VERB'), (\"l'\", 'DET'), ('échéance', 'NOUN'), ('fatidique', 'ADJ'), (',', 'PUNCT'), ('le', 'DET'), ('temps', 'NOUN'), ('de', 'ADP'), ('trouver', 'VERB'), ('une', 'DET'), ('solution', 'NOUN'), ('à', 'ADP'), ('ses', 'DET'), ('problèmes', 'NOUN'), ('métaphysiques', 'ADJ'), (':', 'PUNCT'), ('Dieu', 'PROPN'), ('existe', 'VERB'), ('-il', 'PRON'), ('?', 'PUNCT')]\n",
      "[('Créée', 'VERB'), ('au', '_'), ('à', 'ADP'), ('le', 'DET'), ('cours', 'NOUN'), ('du', '_'), ('de', 'ADP'), ('le', 'DET'), ('troisième', 'ADJ'), ('trimestre', 'NOUN'), ('1915', 'NUM'), ('comme', 'ADP'), ('escadrille', 'NOUN'), ('MF', 'PROPN'), ('93', 'PROPN'), (',', 'PUNCT'), ('elle', 'PRON'), ('a', 'AUX'), ('disparu', 'VERB'), ('en', 'ADP'), ('même', 'ADJ'), ('temps', 'NOUN'), ('que', 'ADP'), ('la', 'DET'), ('30e', 'ADJ'), ('Escadre', 'NOUN'), ('de', 'ADP'), ('Chasse', 'NOUN'), ('à', 'ADP'), ('laquelle', 'PRON'), ('elle', 'PRON'), ('était', 'AUX'), ('intégrée', 'ADJ'), ('.', 'PUNCT')]\n",
      "[('On', 'PRON'), ('ne', 'ADV'), ('peut', 'VERB'), ('éviter', 'VERB'), ('de', 'ADP'), ('penser', 'VERB'), ('à', 'ADP'), (\"l'\", 'DET'), ('actualité', 'NOUN'), ('caractérisée', 'VERB'), ('par', 'ADP'), (\"l'\", 'DET'), ('enlèvement', 'NOUN'), ('des', '_'), ('de', 'ADP'), ('les', 'DET'), ('otages', 'NOUN'), ('au', '_'), ('à', 'ADP'), ('le', 'DET'), ('Niger', 'PROPN'), ('--', 'PUNCT'), ('dont', 'PRON'), ('cinq', 'NUM'), ('Français', 'NOUN'), ('.', 'PUNCT')]\n",
      "[('Elle', 'PRON'), ('est', 'AUX'), ('déjà', 'ADV'), ('soumise', 'VERB'), ('à', 'ADP'), ('rude', 'ADJ'), ('épreuve', 'NOUN'), ('avant', 'ADP'), ('même', 'ADV'), (\"l'\", 'DET'), ('avènement', 'NOUN'), ('de', 'ADP'), ('Flann', 'PROPN'), ('Sinna', 'PROPN'), ('.', 'PUNCT')]\n",
      "[('\"', 'PUNCT'), ('Malgré', 'ADP'), ('les', 'DET'), ('efforts', 'NOUN'), ('consentis', 'VERB'), ('par', 'ADP'), ('quelques', 'DET'), ('pays', 'NOUN'), (',', 'PUNCT'), ('environ', 'ADV'), ('155', 'NUM'), ('millions', 'NOUN'), ('de', 'ADP'), ('personnes', 'NOUN'), ('--', 'PUNCT'), ('soit', 'SCONJ'), ('39', 'NUM'), ('%', 'SYM'), ('de', 'ADP'), ('la', 'DET'), ('population', 'NOUN'), ('--', 'PUNCT'), (\"n'\", 'ADV'), ('ont', 'VERB'), ('pas', 'ADV'), ('encore', 'ADV'), ('accès', 'NOUN'), ('à', 'ADP'), (\"l'\", 'DET'), ('eau', 'NOUN'), ('potable', 'ADJ'), ('en', 'ADP'), ('Afrique', 'PROPN'), ('de', 'ADP'), (\"l'\", 'DET'), ('Ouest', 'PROPN'), ('et', 'CCONJ'), ('du', '_'), ('de', 'ADP'), ('le', 'DET'), ('Centre', 'PROPN'), ('\"', 'PUNCT'), (',', 'PUNCT'), ('selon', 'ADP'), ('le', 'DET'), ('texte', 'NOUN'), ('.', 'PUNCT')]\n",
      "[('La', 'DET'), ('direction', 'NOUN'), ('des', '_'), ('de', 'ADP'), ('les', 'DET'), ('fibres', 'NOUN'), ('est', 'AUX'), ('donc', 'ADV'), ('la', 'DET'), ('direction', 'NOUN'), ('de', 'ADP'), ('plus', 'ADV'), ('grande', 'ADJ'), ('diffusivité', 'NOUN'), (',', 'PUNCT'), ('indiquée', 'VERB'), ('par', 'ADP'), ('le', 'DET'), ('vecteur', 'NOUN'), ('propre', 'ADJ'), ('associé', 'VERB'), ('à', 'ADP'), ('la', 'DET'), ('plus', 'ADV'), ('grande', 'ADJ'), ('valeur', 'NOUN'), ('propre', 'ADJ'), ('du', '_'), ('de', 'ADP'), ('le', 'DET'), ('tenseur', 'NOUN'), ('de', 'ADP'), ('diffusion', 'NOUN'), ('.', 'PUNCT')]\n",
      "[('Les', 'DET'), ('experts', 'NOUN'), ('sont', 'AUX'), ('unanimes', 'ADJ'), ('pour', 'ADP'), ('dater', 'VERB'), ('ce', 'DET'), ('manuscrit', 'NOUN'), ('du', '_'), ('de', 'ADP'), ('le', 'DET'), ('VIe', 'ADJ'), ('siècle', 'NOUN'), ('.', 'PUNCT')]\n",
      "[('Elle', 'PRON'), (\"s'\", 'PRON'), ('étend', 'VERB'), ('sur', 'ADP'), ('105,3', 'NUM'), ('km²', 'NOUN'), ('et', 'CCONJ'), ('comptait', 'VERB'), ('5633', 'NUM'), ('habitants', 'NOUN'), ('en', 'ADP'), ('2010', 'NUM'), ('.', 'PUNCT')]\n",
      "[(\"L'\", 'DET'), ('affaire', 'NOUN'), ('se', 'PRON'), ('complique', 'VERB'), ('un', 'DET'), ('peu', 'ADV'), ('avec', 'ADP'), ('les', 'DET'), ('jeux', 'NOUN'), ('Super', 'PROPN'), ('Nintendo', 'PROPN'), ('ou', 'CCONJ'), (\"d'\", 'ADP'), ('autres', 'ADJ'), ('systèmes', 'NOUN'), (',', 'PUNCT'), ('qui', 'PRON'), ('stockent', 'VERB'), ('les', 'DET'), ('valeurs', 'NOUN'), ('RVB', 'X'), ('absolues', 'ADJ'), ('des', '_'), ('de', 'ADP'), ('les', 'DET'), ('couleurs', 'NOUN'), ('.', 'PUNCT')]\n",
      "[('La', 'DET'), ('cathédrale', 'NOUN'), (',', 'PUNCT'), (\"l'\", 'PART'), ('une', 'PRON'), ('des', '_'), ('de', 'ADP'), ('les', 'DET'), ('plus', 'ADV'), ('vastes', 'ADJ'), ('églises', 'NOUN'), ('du', '_'), ('de', 'ADP'), ('le', 'DET'), ('monde', 'NOUN'), ('arménien', 'ADJ'), (',', 'PUNCT'), ('est', 'AUX'), ('bâtie', 'VERB'), ('sur', 'ADP'), ('le', 'DET'), ('modèle', 'NOUN'), ('de', 'ADP'), ('la', 'DET'), ('cathédrale', 'NOUN'), (\"d'\", 'ADP'), ('Etchmiadzin', 'PROPN'), (':', 'PUNCT'), ('il', 'PRON'), (\"s'\", 'PRON'), ('agit', 'VERB'), (\"d'\", 'ADP'), ('une', 'DET'), ('croix', 'NOUN'), ('libre', 'ADJ'), ('dotée', 'VERB'), ('de', 'ADP'), ('quatre', 'NUM'), ('absides', 'NOUN'), ('.', 'PUNCT')]\n",
      "[('ARF', 'PROPN'), ('(', 'PUNCT'), ('Application', 'NOUN'), ('Roll-over', 'NOUN'), ('Facility', 'NOUN'), (')', 'PUNCT'), ('est', 'AUX'), ('un', 'DET'), ('logiciel', 'NOUN'), ('de', 'ADP'), ('haute', 'ADJ'), ('disponibilité', 'NOUN'), (',', 'PUNCT'), ('fonctionnant', 'VERB'), ('sous', 'ADP'), ('AIX', 'PROPN'), ('sur', 'ADP'), ('les', 'DET'), ('machines', 'NOUN'), ('Bull', 'PROPN'), ('Escala', 'PROPN'), ('ou', 'CCONJ'), ('Linux', 'PROPN'), ('sur', 'ADP'), ('les', 'DET'), ('serveurs', 'NOUN'), ('Bull', 'PROPN'), ('Novascale', 'PROPN'), ('.', 'PUNCT')]\n",
      "[(\"L'\", 'DET'), ('Abbé', 'PROPN'), ('Othon', 'PROPN'), ('y', 'ADV'), ('envoie', 'VERB'), ('Albéric', 'PROPN'), ('accompagné', 'VERB'), ('de', 'ADP'), ('quelques', 'DET'), ('moines', 'NOUN'), ('.', 'PUNCT')]\n",
      "[('Les', 'DET'), ('femelles', 'NOUN'), ('peuvent', 'VERB'), ('réaliser', 'VERB'), ('une', 'NUM'), ('à', 'ADP'), ('deux', 'NUM'), ('portées', 'NOUN'), ('par', 'ADP'), ('an', 'NOUN'), ('du', '_'), ('de', 'ADP'), ('le', 'DET'), ('printemps', 'NOUN'), ('à', 'ADP'), (\"l'\", 'DET'), ('automne', 'NOUN'), ('.', 'PUNCT')]\n",
      "[('Il', 'PRON'), (\"s'\", 'PRON'), ('agit', 'VERB'), ('du', '_'), ('de', 'ADP'), ('le', 'DET'), ('17e', 'ADJ'), ('but', 'NOUN'), ('de', 'ADP'), ('la', 'DET'), ('saison', 'NOUN'), ('pour', 'ADP'), (\"l'\", 'DET'), ('attaquant', 'NOUN'), ('finlandais', 'ADJ'), ('.', 'PUNCT')]\n",
      "[('Condamné', 'VERB'), ('à', 'ADP'), ('dix', 'NUM'), ('ans', 'NOUN'), (',', 'PUNCT'), ('il', 'PRON'), ('passe', 'VERB'), ('le', 'DET'), ('reste', 'NOUN'), ('de', 'ADP'), ('sa', 'DET'), ('vie', 'NOUN'), ('en', 'ADP'), ('prison', 'NOUN'), ('.', 'PUNCT')]\n",
      "[('Stephen', 'PROPN'), ('accuse', 'VERB'), ('Adeang', 'PROPN'), ('et', 'CCONJ'), ('les', 'DET'), ('autres', 'ADJ'), ('députés', 'NOUN'), ('de', 'ADP'), (\"l'\", 'DET'), ('opposition', 'NOUN'), (\"d'\", 'ADP'), ('avoir', 'AUX'), ('voté', 'VERB'), ('la', 'DET'), ('loi', 'NOUN'), ('«', 'PUNCT'), ('après', 'ADP'), ('le', 'DET'), ('coucher', 'NOUN'), ('du', '_'), ('de', 'ADP'), ('le', 'DET'), ('soleil', 'NOUN'), (',', 'PUNCT'), ('le', 'DET'), ('samedi', 'NOUN'), ('de', 'ADP'), ('Pâques', 'PROPN'), ('»', 'PUNCT'), (',', 'PUNCT'), ('«', 'PUNCT'), ('à', 'ADP'), ('la', 'DET'), ('lueur', 'NOUN'), ('des', '_'), ('de', 'ADP'), ('les', 'DET'), ('bougies', 'NOUN'), ('»', 'PUNCT'), ('.', 'PUNCT')]\n",
      "[('Des', 'DET'), ('panneaux', 'NOUN'), ('solaires', 'ADJ'), ('servent', 'VERB'), ('également', 'ADV'), ('à', 'ADP'), ('chauffer', 'VERB'), (\"l'\", 'DET'), ('eau', 'NOUN'), ('chaude', 'ADJ'), ('sanitaire', 'ADJ'), ('.', 'PUNCT')]\n",
      "[('Au', '_'), ('à', 'ADP'), ('le', 'DET'), ('contraire', 'NOUN'), (',', 'PUNCT'), ('la', 'DET'), ('rifampicine', 'NOUN'), ('diminue', 'VERB'), ('son', 'DET'), ('taux', 'NOUN'), ('sérique', 'ADJ'), ('.', 'PUNCT')]\n",
      "[('Ce', 'DET'), ('mécanisme', 'NOUN'), ('peut', 'VERB'), ('permettre', 'VERB'), ('à', 'ADP'), ('la', 'DET'), ('fraude', 'NOUN'), ('de', 'ADP'), ('durer', 'VERB'), ('plusieurs', 'DET'), ('années', 'NOUN'), (';', 'PUNCT')]\n",
      "[('Vous', 'PRON'), ('pensez', 'VERB'), ('que', 'SCONJ'), ('les', 'DET'), ('Mauriciens', 'PROPN'), ('ne', 'ADV'), ('parlent', 'VERB'), ('pas', 'ADV'), ('suffisamment', 'ADV'), ('de', 'ADP'), ('la', 'DET'), ('situation', 'NOUN'), ('linguistique', 'ADJ'), ('de', 'ADP'), ('leur', 'DET'), ('pays', 'NOUN'), ('?', 'PUNCT')]\n",
      "[('Tout', 'DET'), ('ceci', 'PRON'), ('nous', 'PRON'), ('conduit', 'VERB'), ('au', '_'), ('à', 'ADP'), ('le', 'DET'), ('désagréable', 'ADJ'), ('sentiment', 'NOUN'), ('que', 'SCONJ'), (\"l'\", 'DET'), ('économie', 'NOUN'), ('mondiale', 'ADJ'), ('se', 'PRON'), ('dirige', 'VERB'), ('tout', 'ADV'), ('droit', 'ADJ'), ('vers', 'ADP'), ('la', 'DET'), ('déflation', 'NOUN'), ('.', 'PUNCT')]\n",
      "[('Les', 'DET'), ('hameaux', 'NOUN'), ('de', 'ADP'), ('Faÿ', 'PROPN'), ('et', 'CCONJ'), ('de', 'ADP'), ('Barbizon', 'PROPN'), ('lui', 'PRON'), ('étaient', 'AUX'), ('reliés', 'VERB'), ('par', 'ADP'), ('le', 'DET'), ('\"', 'PUNCT'), ('chemin', 'NOUN'), ('de', 'ADP'), ('la', 'DET'), ('Messe', 'PROPN'), ('\"', 'PUNCT'), ('toujours', 'ADV'), ('existant', 'ADJ'), ('.', 'PUNCT')]\n",
      "[('De', 'ADP'), ('plus', 'ADV'), (\"l'\", 'DET'), ('appartenance', 'NOUN'), ('à', 'ADP'), (\"l'\", 'DET'), ('ordre', 'NOUN'), ('équestre', 'ADJ'), ('était', 'AUX'), ('nécessaire', 'ADJ'), ('pour', 'ADP'), ('accéder', 'VERB'), ('aux', '_'), ('à', 'ADP'), ('les', 'DET'), ('postes', 'NOUN'), (\"d'\", 'ADP'), ('officier', 'NOUN'), ('dans', 'ADP'), (\"l'\", 'DET'), ('armée', 'NOUN'), ('.', 'PUNCT')]\n",
      "[('Le', 'DET'), ('Faune', 'PROPN'), ('canarien', 'ADJ'), ('vole', 'VERB'), ('en', 'ADP'), ('une', 'DET'), ('génération', 'NOUN'), ('entre', 'ADP'), ('juin', 'NOUN'), ('et', 'CCONJ'), ('septembre', 'NOUN'), ('suivant', 'ADP'), ('la', 'DET'), ('sous-espèce', 'NOUN'), ('et', 'CCONJ'), (\"l'\", 'DET'), ('altitude', 'NOUN'), ('.', 'PUNCT')]\n",
      "[('Étant', 'AUX'), ('atteint', 'VERB'), ('du', '_'), ('de', 'ADP'), ('le', 'DET'), ('syndrome', 'NOUN'), ('de', 'ADP'), ('Marfan', 'PROPN'), (',', 'PUNCT'), ('il', 'PRON'), ('est', 'AUX'), ('très', 'ADV'), ('grand', 'ADJ'), ('et', 'CCONJ'), ('mince', 'ADJ'), ('.', 'PUNCT')]\n",
      "[('Il', 'PRON'), ('faut', 'VERB'), ('attendre', 'VERB'), ('1527', 'NUM'), ('pour', 'ADP'), ('que', 'SCONJ'), ('nous', 'PRON'), ('parvienne', 'VERB'), ('le', 'DET'), ('plus', 'ADV'), ('ancien', 'ADJ'), ('texte', 'NOUN'), (',', 'PUNCT'), ('écrit', 'VERB'), ('par', 'ADP'), ('Jan', 'PROPN'), ('Ymbrechts', 'PROPN'), ('--', 'PUNCT'), ('sacristain', 'NOUN'), (\"d'\", 'ADP'), ('Evere', 'PROPN'), ('--', 'PUNCT'), (',', 'PUNCT'), ('concernant', 'VERB'), (\"l'\", 'DET'), ('adjudication', 'NOUN'), (\"d'\", 'ADP'), ('un', 'DET'), ('nouveau', 'ADJ'), ('chœur', 'NOUN'), ('à', 'ADP'), ('cette', 'DET'), ('église', 'NOUN'), ('.', 'PUNCT')]\n",
      "[(\"D'\", 'ADP'), ('un', 'DET'), ('coup', 'NOUN'), ('de', 'ADP'), ('pied', 'NOUN'), (',', 'PUNCT'), ('il', 'PRON'), ('le', 'PRON'), ('fait', 'AUX'), ('tomber', 'VERB'), ('de', 'ADP'), ('la', 'DET'), ('main', 'NOUN'), ('du', '_'), ('de', 'ADP'), ('le', 'DET'), ('prêtre', 'NOUN'), (',', 'PUNCT'), ('par', 'ADP'), ('terre', 'NOUN'), ('.', 'PUNCT')]\n",
      "[(\"L'\", 'DET'), ('A887', 'NOUN'), ('et', 'CCONJ'), (\"l'\", 'DET'), ('A87', 'NOUN'), ('traversent', 'VERB'), ('Glenmoriston', 'PROPN'), ('.', 'PUNCT')]\n",
      "[('Le', 'DET'), ('cours', 'NOUN'), ('de', 'ADP'), ('dessin', 'NOUN'), ('est', 'AUX'), ('il', 'PRON'), ('pour', 'ADP'), ('les', 'DET'), ('débutants', 'NOUN'), ('?', 'PUNCT')]\n",
      "[('Bien', 'ADV'), ('que', 'SCONJ'), ('cette', 'DET'), ('entreprise', 'NOUN'), ('ait', 'AUX'), ('été', 'AUX'), ('considérée', 'VERB'), ('comme', 'ADP'), ('sans', 'ADP'), ('espoir', 'NOUN'), (',', 'PUNCT'), ('le', 'DET'), ('syndicat', 'NOUN'), ('persévéra', 'VERB'), (',', 'PUNCT'), ('même', 'ADV'), (\"s'\", 'SCONJ'), ('il', 'PRON'), ('ne', 'ADV'), ('pouvait', 'VERB'), ('payer', 'VERB'), ('que', 'ADV'), ('huit', 'NUM'), ('jours', 'NOUN'), ('de', 'ADP'), ('recherches', 'NOUN'), ('.', 'PUNCT')]\n",
      "[('Tous', 'DET'), ('les', 'DET'), ('matchs', 'NOUN'), ('sont', 'AUX'), ('désignés', 'VERB'), ('par', 'ADP'), ('tirage', 'NOUN'), ('au', '_'), ('à', 'ADP'), ('le', 'DET'), ('sort', 'NOUN'), ('.', 'PUNCT')]\n",
      "[('Larry', 'PROPN'), ('Harlow', 'PROPN'), ('a', 'AUX'), ('fait', 'AUX'), ('découvrir', 'VERB'), ('le', 'DET'), ('trompettiste', 'NOUN'), ('cubain', 'ADJ'), ('Alfredo', 'PROPN'), ('\"', 'PUNCT'), ('Chocolate', 'NOUN'), ('\"', 'PUNCT'), ('Armenteros', 'PROPN'), ('et', 'CCONJ'), ('le', 'DET'), ('tromboniste', 'NOUN'), ('Mark', 'PROPN'), ('Weinstein', 'PROPN'), (',', 'PUNCT'), ('qui', 'PRON'), ('a', 'AUX'), ('co-arrangé', 'VERB'), (\"l'\", 'DET'), ('album', 'NOUN'), ('avec', 'ADP'), ('lui', 'PRON'), ('.', 'PUNCT')]\n",
      "[('Depuis', 'ADP'), ('1986', 'NUM'), (',', 'PUNCT'), ('les', 'DET'), ('émissions', 'NOUN'), ('de', 'ADP'), ('Radio', 'PROPN'), ('Chine', 'PROPN'), ('Internationale', 'PROPN'), ('sont', 'AUX'), ('relayées', 'VERB'), ('par', 'ADP'), ('les', 'DET'), ('émetteurs', 'NOUN'), ('français', 'ADJ'), (\"d'\", 'ADP'), ('Issoudun', 'PROPN'), ('(', 'PUNCT'), ('en', 'ADP'), ('métropole', 'NOUN'), (')', 'PUNCT'), ('et', 'CCONJ'), ('de', 'ADP'), ('Montsinéry-Tonnegrande', 'PROPN'), ('(', 'PUNCT'), ('en', 'ADP'), ('Guyane', 'PROPN'), (')', 'PUNCT'), ('.', 'PUNCT')]\n",
      "[('Englebert', 'PROPN'), ('exprime', 'VERB'), ('son', 'DET'), ('hostilité', 'NOUN'), ('et', 'CCONJ'), ('semble', 'VERB'), ('soutenu', 'VERB'), ('par', 'ADP'), ('le', 'DET'), ('Cardinal', 'PROPN'), ('Mercier', 'PROPN'), ('.', 'PUNCT')]\n",
      "[('Commandité', 'VERB'), ('par', 'ADP'), ('son', 'DET'), ('oncle', 'NOUN'), (',', 'PUNCT'), ('le', 'DET'), ('directeur', 'NOUN'), ('du', '_'), ('de', 'ADP'), ('le', 'DET'), ('journal', 'NOUN'), ('«', 'PUNCT'), ('La', 'DET'), ('Tribune', 'PROPN'), ('»', 'PUNCT'), (',', 'PUNCT'), ('la', 'DET'), ('journaliste', 'NOUN'), ('Diana', 'PROPN'), ('Palmer', 'PROPN'), ('se', 'PRON'), ('rend', 'VERB'), ('sur', 'ADP'), (\"l'\", 'DET'), ('île', 'NOUN'), ('de', 'ADP'), ('Benalla', 'PROPN'), ('et', 'CCONJ'), ('est', 'AUX'), ('kidnappée', 'VERB'), ('par', 'ADP'), ('des', 'DET'), ('pirates', 'NOUN'), ('...', 'PUNCT')]\n",
      "[('Un', 'DET'), ('membre', 'NOUN'), ('du', '_'), ('de', 'ADP'), ('le', 'DET'), ('gouvernement', 'NOUN'), ('peut', 'VERB'), ('également', 'ADV'), ('servir', 'VERB'), ('de', 'ADP'), ('porte-parole', 'NOUN'), ('de', 'ADP'), (\"l'\", 'DET'), ('exécutif', 'NOUN'), ('.', 'PUNCT')]\n",
      "[('Ironiquement', 'ADV'), (',', 'PUNCT'), ('Trois', 'NUM'), ('morceaux', 'NOUN'), ('en', 'ADP'), ('forme', 'NOUN'), ('de', 'ADP'), ('poire', 'NOUN'), ('comporte', 'VERB'), ('sept', 'NUM'), ('mouvements', 'NOUN'), ('.', 'PUNCT')]\n",
      "[('Elles', 'PRON'), ('faisaient', 'VERB'), (\"l'\", 'DET'), ('objet', 'NOUN'), (\"d'\", 'ADP'), ('un', 'DET'), ('ordre', 'NOUN'), ('de', 'ADP'), ('recherche', 'NOUN'), ('et', 'CCONJ'), (\"d'\", 'ADP'), ('arrestation', 'NOUN'), ('pour', 'ADP'), ('avoir', 'AUX'), ('participé', 'VERB'), ('au', '_'), ('à', 'ADP'), ('le', 'DET'), ('camp', 'NOUN'), ('--', 'PUNCT'), ('également', 'ADV'), ('dénommé', 'VERB'), ('de', 'ADP'), ('la', 'DET'), ('Dignité', 'PROPN'), ('--', 'PUNCT'), ('et', 'CCONJ'), ('aux', '_'), ('à', 'ADP'), ('les', 'DET'), ('affrontements', 'NOUN'), ('postérieurs', 'ADJ'), ('qui', 'PRON'), ('ont', 'AUX'), ('eu', 'VERB'), ('lieu', 'NOUN'), ('dans', 'ADP'), ('les', 'DET'), ('rues', 'NOUN'), ('de', 'ADP'), ('la', 'DET'), ('ville', 'NOUN'), ('.', 'PUNCT')]\n",
      "[('Ce', 'DET'), ('test', 'NOUN'), ('intéresserait', 'VERB'), ('plutôt', 'ADV'), ('les', 'DET'), ('ingénieurs', 'NOUN'), ('qui', 'PRON'), ('songent', 'VERB'), ('à', 'ADP'), ('intégrer', 'VERB'), ('ce', 'DET'), ('moteur', 'NOUN'), ('dans', 'ADP'), ('leurs', 'DET'), ('propres', 'ADJ'), ('développements', 'NOUN'), ('.', 'PUNCT')]\n",
      "[('Jabba', 'PROPN'), ('mène', 'VERB'), ('une', 'DET'), ('vie', 'NOUN'), ('de', 'ADP'), ('plaisirs', 'NOUN'), ('dans', 'ADP'), ('son', 'DET'), ('palais', 'NOUN'), (',', 'PUNCT'), ('et', 'CCONJ'), ('reçoit', 'VERB'), ('les', 'DET'), ('visites', 'NOUN'), ('de', 'ADP'), ('célèbres', 'ADJ'), ('contrebandiers', 'NOUN'), ('.', 'PUNCT')]\n",
      "[('Toujours', 'ADV'), ('en', 'ADP'), ('Géométrie', 'NOUN'), ('euclidienne', 'ADJ'), (',', 'PUNCT'), ('on', 'PRON'), ('montre', 'VERB'), ('que', 'SCONJ'), ('dans', 'ADP'), ('un', 'DET'), ('triangle', 'NOUN'), (',', 'PUNCT'), ('le', 'DET'), ('centre', 'NOUN'), ('de', 'ADP'), ('gravité', 'NOUN'), ('G', 'X'), (',', 'PUNCT'), (\"l'\", 'DET'), ('orthocentre', 'NOUN'), ('H', 'SYM'), (',', 'PUNCT'), ('le', 'DET'), ('centre', 'NOUN'), ('du', '_'), ('de', 'ADP'), ('le', 'DET'), ('cercle', 'NOUN'), ('circonscrit', 'VERB'), ('Ω', 'X'), (',', 'PUNCT'), ('et', 'CCONJ'), ('le', 'DET'), ('centre', 'NOUN'), ('du', '_'), ('de', 'ADP'), ('le', 'DET'), ('cercle', 'NOUN'), (\"d'\", 'ADP'), ('Euler', 'PROPN'), ('E', 'X'), ('sont', 'AUX'), ('tous', 'DET'), ('les', 'DET'), ('quatre', 'PRON'), ('alignés', 'VERB'), ('sur', 'ADP'), ('une', 'DET'), ('droite', 'NOUN'), ('dite', 'VERB'), ('droite', 'NOUN'), (\"d'\", 'ADP'), ('Euler', 'PROPN'), ('du', '_'), ('de', 'ADP'), ('le', 'DET'), ('triangle', 'NOUN'), (',', 'PUNCT'), ('de', 'ADP'), ('plus', 'ADV'), ('ils', 'PRON'), ('sont', 'VERB'), ('en', 'ADP'), ('division', 'NOUN'), ('harmonique', 'ADJ'), ('(', 'PUNCT'), ('dans', 'ADP'), ('cet', 'DET'), ('ordre', 'NOUN'), (')', 'PUNCT'), ('.', 'PUNCT')]\n",
      "[('Le', 'DET'), ('27', 'NUM'), ('mai', 'NOUN'), ('2011', 'NUM'), (',', 'PUNCT'), ('le', 'DET'), ('Président', 'PROPN'), ('chargé', 'VERB'), ('de', 'ADP'), (\"l'\", 'DET'), ('instruction', 'NOUN'), ('au', '_'), ('à', 'ADP'), ('le', 'DET'), ('Tribunal', 'PROPN'), ('de', 'ADP'), ('grande', 'ADJ'), ('instance', 'NOUN'), ('de', 'ADP'), ('Paris', 'PROPN'), ('rend', 'VERB'), ('une', 'DET'), ('ordonnance', 'NOUN'), ('de', 'ADP'), ('non-lieu', 'NOUN'), ('pour', 'ADP'), ('charges', 'NOUN'), ('insuffisantes', 'ADJ'), ('à', 'ADP'), (\"l'\", 'DET'), ('encontre', 'NOUN'), (\"d'\", 'ADP'), ('Yves', 'PROPN'), ('Poey', 'PROPN'), ('.', 'PUNCT')]\n",
      "[('Ses', 'DET'), ('habitants', 'NOUN'), ('sont', 'AUX'), ('appelés', 'VERB'), ('les', 'DET'), ('Paydrets', 'PROPN'), ('et', 'CCONJ'), ('les', 'DET'), ('Paydrètes', 'PROPN'), (';', 'PUNCT')]\n",
      "[('Bien', 'ADV'), ('évidemment', 'ADV'), ('tous', 'DET'), ('les', 'DET'), ('matériaux', 'NOUN'), ('de', 'ADP'), ('valeur', 'NOUN'), ('avaient', 'AUX'), ('été', 'AUX'), ('prélevés', 'VERB'), ('depuis', 'ADP'), ('longtemps', 'ADV'), (',', 'PUNCT'), ('cependant', 'ADV'), ('un', 'DET'), ('certain', 'ADJ'), ('nombre', 'NOUN'), (\"d'\", 'ADP'), ('artefacts', 'NOUN'), ('caractéristiques', 'ADJ'), ('du', '_'), ('de', 'ADP'), ('le', 'DET'), ('viatique', 'NOUN'), ('funéraire', 'ADJ'), ('royal', 'ADJ'), ('de', 'ADP'), ('la', 'DET'), ('XVIIIe', 'ADJ'), ('dynastie', 'NOUN'), ('ont', 'AUX'), ('pu', 'VERB'), ('être', 'AUX'), ('retrouvés', 'VERB'), ('et', 'CCONJ'), ('dont', 'PRON'), ('le', 'DET'), ('tombeau', 'NOUN'), ('de', 'ADP'), ('Toutânkhamon', 'PROPN'), ('livrera', 'VERB'), ('plus', 'ADV'), ('tard', 'ADV'), ('un', 'DET'), ('ensemble', 'NOUN'), ('quasi', 'ADV'), ('complet', 'ADJ'), ('.', 'PUNCT')]\n",
      "[('Il', 'PRON'), ('a', 'AUX'), ('sauté', 'VERB'), ('sur', 'ADP'), ('Mourinho', 'PROPN'), (',', 'PUNCT'), ('en', 'ADP'), ('demandant', 'VERB'), ('à', 'ADP'), ('haute', 'ADJ'), ('voix', 'NOUN'), ('au', '_'), ('à', 'ADP'), ('le', 'DET'), ('Portugais', 'PROPN'), ('de', 'ADP'), ('respecter', 'VERB'), ('les', 'DET'), ('règles', 'NOUN'), ('et', 'CCONJ'), ('de', 'ADP'), ('rester', 'VERB'), ('dans', 'ADP'), ('la', 'DET'), ('zone', 'NOUN'), ('de', 'ADP'), ('direction', 'NOUN'), ('pour', 'ADP'), ('entraîneurs', 'NOUN'), ('.', 'PUNCT')]\n",
      "[('Elle', 'PRON'), ('reste', 'VERB'), ('actuellement', 'ADV'), ('dans', 'ADP'), (\"l'\", 'DET'), ('attente', 'NOUN'), ('du', '_'), ('de', 'ADP'), ('le', 'DET'), ('verdict', 'NOUN'), ('final', 'ADJ'), ('.', 'PUNCT')]\n",
      "[('Des', 'DET'), ('innocents', 'NOUN'), ('se', 'PRON'), ('trouvent', 'VERB'), ('impliqués', 'VERB'), ('dans', 'ADP'), ('des', 'DET'), ('affaires', 'NOUN'), ('comme', 'ADP'), ('celle-ci', 'PRON'), ('alors', 'ADV'), (\"qu'\", 'SCONJ'), ('ils', 'PRON'), ('sont', 'AUX'), ('en', 'ADP'), ('fait', 'NOUN'), ('utilisés', 'VERB'), ('dans', 'ADP'), ('des', 'DET'), ('stratagèmes', 'NOUN'), ('politiques', 'ADJ'), ('.', 'PUNCT'), ('\"', 'PUNCT')]\n",
      "[('Les', 'DET'), ('soldats', 'NOUN'), ('pillent', 'VERB'), ('les', 'DET'), ('maisons', 'NOUN'), ('.', 'PUNCT')]\n",
      "[('Le', 'DET'), ('secrétaire', 'NOUN'), ('général', 'ADJ'), ('des', '_'), ('de', 'ADP'), ('les', 'DET'), ('nations', 'NOUN'), ('Unies', 'PROPN'), (',', 'PUNCT'), ('Ban', 'PROPN'), ('ki-Moon', 'PROPN'), (',', 'PUNCT'), ('a', 'AUX'), ('soulevé', 'VERB'), (\"d'\", 'ADP'), ('une', 'DET'), ('manière', 'NOUN'), ('claire', 'ADJ'), ('et', 'CCONJ'), ('objective', 'ADJ'), (',', 'PUNCT'), ('dans', 'ADP'), ('son', 'DET'), ('rapport', 'NOUN'), ('publié', 'VERB'), ('vendredi', 'NOUN'), ('et', 'CCONJ'), ('qui', 'PRON'), ('sera', 'AUX'), ('examiné', 'VERB'), ('mardi', 'NOUN'), ('prochain', 'ADJ'), ('par', 'ADP'), ('le', 'DET'), ('Conseil', 'NOUN'), ('de', 'ADP'), ('sécurité', 'NOUN'), (',', 'PUNCT'), (\"l'\", 'DET'), ('urgence', 'NOUN'), ('de', 'ADP'), ('faire', 'VERB'), ('face', 'NOUN'), ('à', 'ADP'), ('la', 'DET'), ('situation', 'NOUN'), ('relative', 'ADJ'), ('aux', '_'), ('à', 'ADP'), ('les', 'DET'), ('violations', 'NOUN'), ('des', '_'), ('de', 'ADP'), ('les', 'DET'), ('droits', 'NOUN'), ('de', 'ADP'), (\"l'\", 'DET'), ('Homme', 'NOUN'), ('au', '_'), ('à', 'ADP'), ('le', 'DET'), ('Sahara', 'PROPN'), ('occidental', 'ADJ'), ('.', 'PUNCT')]\n",
      "[('En', 'ADP'), ('géométrie', 'NOUN'), (',', 'PUNCT'), ('le', 'DET'), ('dodécadodécaèdre', 'NOUN'), ('tronqué', 'VERB'), ('est', 'AUX'), ('un', 'DET'), ('polyèdre', 'NOUN'), ('uniforme', 'ADJ'), ('non-convexe', 'ADJ'), (',', 'PUNCT'), ('indexé', 'VERB'), ('sous', 'ADP'), ('le', 'DET'), ('nom', 'NOUN'), ('U59', 'NOUN'), ('.', 'PUNCT')]\n",
      "[('Tout', 'ADV'), ('en', 'ADP'), ('regrettant', 'VERB'), ('que', 'SCONJ'), ('le', 'DET'), ('Conseil', 'NOUN'), ('de', 'ADP'), ('sécurité', 'NOUN'), ('\"', 'PUNCT'), (\"n'\", 'ADV'), ('assume', 'VERB'), ('toujours', 'ADV'), ('pas', 'ADV'), ('ses', 'DET'), ('responsabilités', 'NOUN'), ('face', 'ADP'), ('à', 'ADP'), ('la', 'DET'), ('tragédie', 'NOUN'), ('syrienne', 'ADJ'), ('\"', 'PUNCT'), (',', 'PUNCT'), ('avec', 'ADP'), ('notamment', 'ADV'), ('le', 'DET'), ('refus', 'NOUN'), ('de', 'ADP'), ('la', 'DET'), ('Russie', 'PROPN'), ('et', 'CCONJ'), ('de', 'ADP'), ('la', 'DET'), ('Chine', 'PROPN'), ('de', 'ADP'), ('voter', 'VERB'), ('des', 'DET'), ('sanctions', 'NOUN'), ('contre', 'ADP'), ('Damas', 'PROPN'), (',', 'PUNCT'), ('il', 'PRON'), ('a', 'AUX'), ('exclu', 'VERB'), ('toute', 'DET'), ('initiative', 'NOUN'), ('française', 'ADJ'), ('en', 'ADP'), ('dehors', 'NOUN'), ('du', '_'), ('de', 'ADP'), ('le', 'DET'), ('cadre', 'NOUN'), ('légal', 'ADJ'), ('des', '_'), ('de', 'ADP'), ('les', 'DET'), ('Nations', 'PROPN'), ('unies', 'ADJ'), ('.', 'PUNCT')]\n",
      "[(\"C'\", 'PRON'), ('est', 'AUX'), (\"l'\", 'PART'), ('une', 'PRON'), ('des', '_'), ('de', 'ADP'), ('les', 'DET'), ('trois', 'NUM'), ('banlieues', 'NOUN'), ('où', 'PRON'), ('ce', 'DET'), ('trafic', 'NOUN'), ('domine', 'VERB'), ('.', 'PUNCT')]\n",
      "[('Elle', 'PRON'), (\"s'\", 'PRON'), ('est', 'AUX'), ('donné', 'VERB'), (\"jusqu'\", 'ADP'), ('à', 'ADP'), ('mi-août', 'NOUN'), ('pour', 'ADP'), ('y', 'PRON'), ('réfléchir', 'VERB'), ('.', 'PUNCT')]\n",
      "[('Il', 'PRON'), ('crée', 'VERB'), ('alors', 'ADV'), ('la', 'DET'), ('vis', 'NOUN'), ('aérienne', 'ADJ'), ('(', 'PUNCT'), ('ou', 'CCONJ'), ('hélice', 'NOUN'), ('ou', 'CCONJ'), ('aile', 'NOUN'), ('tournante', 'ADJ'), ('hélicoïde', 'ADJ'), ('...', 'PUNCT'), (')', 'PUNCT'), ('avec', 'ADP'), ('une', 'DET'), ('simple', 'ADJ'), ('vis', 'NOUN'), ('passée', 'VERB'), ('au', '_'), ('à', 'ADP'), ('le', 'DET'), ('milieu', 'NOUN'), ('de', 'ADP'), ('deux', 'NUM'), ('plaques', 'NOUN'), ('de', 'ADP'), ('bois', 'NOUN'), ('en', 'ADP'), (\"s'\", 'PRON'), ('inspirant', 'VERB'), ('du', '_'), ('de', 'ADP'), ('le', 'DET'), ('principe', 'NOUN'), ('de', 'ADP'), ('la', 'DET'), ('Vis', 'PROPN'), (\"d'\", 'ADP'), ('Archimède', 'PROPN'), (',', 'PUNCT'), ('utilisée', 'VERB'), ('dès', 'ADP'), (\"l'\", 'DET'), ('antiquité', 'NOUN'), ('pour', 'ADP'), ('monter', 'VERB'), ('de', 'ADP'), (\"l'\", 'DET'), ('eau', 'NOUN'), ('.', 'PUNCT')]\n",
      "[('La', 'DET'), ('premiers', 'ADJ'), ('AVN', 'PROPN'), ('Awards', 'PROPN'), ('sont', 'AUX'), ('décernés', 'VERB'), ('en', 'ADP'), ('février', 'NOUN'), ('1984', 'NUM'), (',', 'PUNCT'), ('au', '_'), ('à', 'ADP'), ('le', 'DET'), ('titre', 'NOUN'), ('de', 'ADP'), (\"l'\", 'DET'), ('année', 'NOUN'), ('1983', 'NUM'), ('.', 'PUNCT')]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Inconsistent number of columns:\n1\tÀ\tà\tADP\t_\t_\t6\tcase\t_\t_\r\n2\tpartir\tpartir\tVERB\t_\tVerbForm=Inf\t1\tfixed\t_\t_\r\n3-4\tdu\t_\t_\t_\t_\t_\t_\t_\t_\r\n3\tde\tde\tADP\t_\t_\t1\tfixed\t_\t_\r\n4\tle\tle\tDET\t_\tDefinite=Def|Gender=Masc|Number=Sing|PronType=Art\t6\tdet\t_\t_\r\n5\tXXIe\tXXIe\tADJ\t_\tGender=Masc|Number=Sing|NumType=Ord\t6\tamod\t_\t_\r\n6\tsiècle\tsiècle\tNOUN\t_\tGender=Masc|Number=Sing\t19\tobl\t_\tSpaceAfter=No\r\n7\t,\t,\tPUNCT\t_\t_\t19\tpunct\t_\t_\r\n8\tles\tle\tDET\t_\tDefinite=Def|Gender=Masc|Number=Plur|PronType=Art\t9\tdet\t_\t_\r\n9\trecensements\trecensement\tNOUN\t_\tGender=Masc|Number=Plur\t19\tnsubj\t_\t_\r\n10\tréels\tréel\tADJ\t_\tGender=Masc|Number=Plur\t9\tamod\t_\t_\r\n11-12\tdes\t_\t_\t_\t_\t_\t_\t_\t_\r\n11\tde\tde\tADP\t_\t_\t13\tcase\t_\t_\r\n12\tles\tle\tDET\t_\tDefinite=Def|Gender=Fem|Number=Plur|PronType=Art\t13\tdet\t_\t_\r\n13\tcommunes\tcommune\tNOUN\t_\tGender=Fem|Number=Plur\t9\tnmod\t_\t_\r\n14\tde\tde\tADP\t_\t_\t18\tcase\t_\t_\r\n15\tmoins\tmoins\tADV\t_\t_\t18\tdet\t_\t_\r\n16\tde\tde\tADP\t_\t_\t15\tfixed\t_\t_\r\n17\t10 000\t10 000\tNUM\t_\t_\t18\tnummod\t_\t_\r\n18\thabitants\thabitant\tNOUN\t_\tGender=Masc|Number=Plur\t13\tnmod\t_\t_\r\n19\tont\tavoir\tVERB\t_\tMood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin\t0\troot\t_\t_\r\n20\tlieu\tlieu\tNOUN\t_\tGender=Masc|Number=Sing\t19\tobj\t_\t_\r\n21\ttous\ttout\tDET\t_\tGender=Masc|Number=Plur\t24\tdet\t_\t_\r\n22\tles\tle\tDET\t_\tDefinite=Def|Gender=Masc|Number=Plur|PronType=Art\t24\tdet\t_\t_\r\n23\tcinq\tcinq\tNUM\t_\t_\t24\tnummod\t_\t_\r\n24\tans\tan\tNOUN\t_\tGender=Masc|Number=Plur\t19\tobl\t_\tSpaceAfter=No\r\n25\t.\t.\tPUNCT\t_\t_\t19\tpunct\t_\t_",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[204], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pos_tag\n\u001b[0;32m      3\u001b[0m sentences \u001b[38;5;241m=\u001b[39m corpus_train\u001b[38;5;241m.\u001b[39mtagged_sents()\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m ptagger\u001b[38;5;241m.\u001b[39mtrain(sentences\u001b[38;5;241m=\u001b[39msentences, nr_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lcsch\\anaconda3\\envs\\data-science\\Lib\\site-packages\\nltk\\collections.py:399\u001b[0m, in \u001b[0;36mLazyMap.iterate_from\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21miterate_from\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;66;03m# Special case: one lazy sublist\u001b[39;00m\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lists) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_all_lazy:\n\u001b[1;32m--> 399\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lists\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterate_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lcsch\\anaconda3\\envs\\data-science\\Lib\\site-packages\\nltk\\corpus\\reader\\util.py:306\u001b[0m, in \u001b[0;36mStreamBackedCorpusView.iterate_from\u001b[1;34m(self, start_tok)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_toknum \u001b[38;5;241m=\u001b[39m toknum\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_blocknum \u001b[38;5;241m=\u001b[39m block_index\n\u001b[1;32m--> 306\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_block\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tokens, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m, AbstractLazySequence)), (\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock reader \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m() should return list or tuple.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_block\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m    310\u001b[0m )\n\u001b[0;32m    311\u001b[0m num_toks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens)\n",
      "File \u001b[1;32mc:\\Users\\lcsch\\anaconda3\\envs\\data-science\\Lib\\site-packages\\nltk\\corpus\\reader\\conll.py:231\u001b[0m, in \u001b[0;36mConllCorpusReader._read_grid_block\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m grid:\n\u001b[0;32m    230\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(row) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(grid[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m--> 231\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInconsistent number of columns:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m block)\n\u001b[0;32m    232\u001b[0m     grids\u001b[38;5;241m.\u001b[39mappend(grid)\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grids\n",
      "\u001b[1;31mValueError\u001b[0m: Inconsistent number of columns:\n1\tÀ\tà\tADP\t_\t_\t6\tcase\t_\t_\r\n2\tpartir\tpartir\tVERB\t_\tVerbForm=Inf\t1\tfixed\t_\t_\r\n3-4\tdu\t_\t_\t_\t_\t_\t_\t_\t_\r\n3\tde\tde\tADP\t_\t_\t1\tfixed\t_\t_\r\n4\tle\tle\tDET\t_\tDefinite=Def|Gender=Masc|Number=Sing|PronType=Art\t6\tdet\t_\t_\r\n5\tXXIe\tXXIe\tADJ\t_\tGender=Masc|Number=Sing|NumType=Ord\t6\tamod\t_\t_\r\n6\tsiècle\tsiècle\tNOUN\t_\tGender=Masc|Number=Sing\t19\tobl\t_\tSpaceAfter=No\r\n7\t,\t,\tPUNCT\t_\t_\t19\tpunct\t_\t_\r\n8\tles\tle\tDET\t_\tDefinite=Def|Gender=Masc|Number=Plur|PronType=Art\t9\tdet\t_\t_\r\n9\trecensements\trecensement\tNOUN\t_\tGender=Masc|Number=Plur\t19\tnsubj\t_\t_\r\n10\tréels\tréel\tADJ\t_\tGender=Masc|Number=Plur\t9\tamod\t_\t_\r\n11-12\tdes\t_\t_\t_\t_\t_\t_\t_\t_\r\n11\tde\tde\tADP\t_\t_\t13\tcase\t_\t_\r\n12\tles\tle\tDET\t_\tDefinite=Def|Gender=Fem|Number=Plur|PronType=Art\t13\tdet\t_\t_\r\n13\tcommunes\tcommune\tNOUN\t_\tGender=Fem|Number=Plur\t9\tnmod\t_\t_\r\n14\tde\tde\tADP\t_\t_\t18\tcase\t_\t_\r\n15\tmoins\tmoins\tADV\t_\t_\t18\tdet\t_\t_\r\n16\tde\tde\tADP\t_\t_\t15\tfixed\t_\t_\r\n17\t10 000\t10 000\tNUM\t_\t_\t18\tnummod\t_\t_\r\n18\thabitants\thabitant\tNOUN\t_\tGender=Masc|Number=Plur\t13\tnmod\t_\t_\r\n19\tont\tavoir\tVERB\t_\tMood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin\t0\troot\t_\t_\r\n20\tlieu\tlieu\tNOUN\t_\tGender=Masc|Number=Sing\t19\tobj\t_\t_\r\n21\ttous\ttout\tDET\t_\tGender=Masc|Number=Plur\t24\tdet\t_\t_\r\n22\tles\tle\tDET\t_\tDefinite=Def|Gender=Masc|Number=Plur|PronType=Art\t24\tdet\t_\t_\r\n23\tcinq\tcinq\tNUM\t_\t_\t24\tnummod\t_\t_\r\n24\tans\tan\tNOUN\t_\tGender=Masc|Number=Plur\t19\tobl\t_\tSpaceAfter=No\r\n25\t.\t.\tPUNCT\t_\t_\t19\tpunct\t_\t_"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "sentences = corpus_train.tagged_sents()\n",
    "\n",
    "# todo : trouver l'erreur ici\n",
    "for sent in sentences:\n",
    "    print(sent)\n",
    "\n",
    "ptagger.train(sentences=sentences, nr_iter=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire ici vos réponses aux questions (temps d'entraînement et taille du modèle).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5c.** Veuillez évaluer le tagger sur les données de test et afficher le taux de correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veuillez écrire votre code ici.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "Veuillez remplir le tableau suivant avec les scores obtenus et discuter brièvement comment se comparent les trois POS taggers sur ces données de test.\n",
    "\n",
    "| spaCy (partie 3) | spaCy (partie 4) | NLTK (partie 5) | \n",
    "|------------------|------------------|-----------------|\n",
    "| tagger fourni    | tagger entraîné  | tagger entraîné |\n",
    "| 0.917              | 0.958              | pas fait             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fin du Labo.** Veuillez nettoyer ce notebook en gardant seulement les résultats désirés, l'enregistrer, et le soumettre comme devoir sur Cyberlearn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
