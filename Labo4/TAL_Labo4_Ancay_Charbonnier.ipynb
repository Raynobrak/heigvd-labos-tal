{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://heig-vd.ch/docs/default-source/doc-global-newsletter/2020-slim.svg\" alt=\"HEIG-VD Logo\" width=\"100\" align=\"right\" /> \n",
    "\n",
    "# Cours TAL - Laboratoire 4<br/>Reconnaissance des entités nommées\n",
    "\n",
    "**Objectif**\n",
    "\n",
    "L'objectif de ce travail est de comparer la reconnaissance des entités nommées (*named entity recognition*, NER) faite par quatre systèmes : NLTK, spaCy (deux modèles, 'en_core_web_sm' et 'en_core_web_lg'), et DistilBERT/NER.  Les données de test en anglais vous sont fournies sur Cyberlearn au format CoNLL.  Pour comparer les systèmes, on utilise la macro-moyenne des scores f1 pour chaque étiquette.  Vous pouvez concevoir l'ensemble du projet par vous-mêmes, ou suivre les indications suivantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NER avec spaCy et NLTK sur un texte court"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm\n",
    "# exécuter la ligne ci-dessus une fois, si nécessaire, idem pour en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"Reinhold Messner made a solo ascent of Mount Everest and was later a member of the European Parliament.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1a.** Veuillez traiter ce texte avec la pipeline 'nlp', et pour chaque entité nommée trouvée veuillez afficher les mots qui la composent et son type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens and their POS tags:\n",
      "Reinhold: PROPN\n",
      "Messner: PROPN\n",
      "made: VERB\n",
      "a: DET\n",
      "solo: ADJ\n",
      "ascent: NOUN\n",
      "of: ADP\n",
      "Mount: PROPN\n",
      "Everest: PROPN\n",
      "and: CCONJ\n",
      "was: AUX\n",
      "later: ADV\n",
      "a: DET\n",
      "member: NOUN\n",
      "of: ADP\n",
      "the: DET\n",
      "European: PROPN\n",
      "Parliament: PROPN\n",
      ".: PUNCT\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(raw_text)\n",
    "\n",
    "print(\"Tokens and their POS tags:\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text}: {token.pos_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('maxent_ne_chunker') \n",
    "#nltk.download('words') \n",
    "# exécuter les deux lignes ci-dessus une fois, si nécessaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1b.** Veuillez effectuer avec NLTK la tokenization, le POS tagging et le *NE chunking* de `raw_text` (voir la [documentation NLTK](https://www.nltk.org/api/nltk.chunk.ne_chunk.html#nltk.chunk.ne_chunk)).  Veuillez afficher le résultat et indiquer son type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "\n",
    "def extract_entities(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Perform POS tagging\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    # Perform Named Entity Recognition\n",
    "    named_entities = ne_chunk(pos_tags)\n",
    "\n",
    "    entities = []\n",
    "    for subtree in named_entities:\n",
    "        if isinstance(subtree, Tree):\n",
    "            entity_name = \" \".join([word for word, tag in subtree.leaves()])\n",
    "            entity_type = subtree.label()\n",
    "            entities.append((entity_name, entity_type))\n",
    "    \n",
    "    return entities, named_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Named Entities:\n",
      "Reinhold: PERSON\n",
      "Messner: PERSON\n",
      "Mount Everest: PERSON\n",
      "European Parliament: ORGANIZATION\n"
     ]
    }
   ],
   "source": [
    "entities, named_entities = extract_entities(raw_text)\n",
    "\n",
    "print(\"\\nNamed Entities:\")\n",
    "for entity in entities:\n",
    "    print(f\"{entity[0]}: {entity[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1c.** Veuillez afficher, pour chaque entité nommée, les mots qui la composent et son type.  Vous pouvez parcourir le résultat précédent avec une boucle `for`, et déterminer si un noeud possède une étiquette avec la fonction `hasattr(noeud, 'label')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(tree, indent=\"  \"):\n",
    "    if isinstance(tree, Tree):\n",
    "        print(indent + str(tree.label()))\n",
    "        for child in tree:\n",
    "            print_tree(child, indent + \"  \")\n",
    "    else:\n",
    "        print(indent + str(tree))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  S\n",
      "    PERSON\n",
      "      ('Reinhold', 'NNP')\n",
      "    PERSON\n",
      "      ('Messner', 'NNP')\n",
      "    ('made', 'VBD')\n",
      "    ('a', 'DT')\n",
      "    ('solo', 'JJ')\n",
      "    ('ascent', 'NN')\n",
      "    ('of', 'IN')\n",
      "    PERSON\n",
      "      ('Mount', 'NNP')\n",
      "      ('Everest', 'NNP')\n",
      "    ('and', 'CC')\n",
      "    ('was', 'VBD')\n",
      "    ('later', 'RB')\n",
      "    ('a', 'DT')\n",
      "    ('member', 'NN')\n",
      "    ('of', 'IN')\n",
      "    ('the', 'DT')\n",
      "    ORGANIZATION\n",
      "      ('European', 'NNP')\n",
      "      ('Parliament', 'NNP')\n",
      "    ('.', '.')\n"
     ]
    }
   ],
   "source": [
    "print_tree(named_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1d.** À ce stade, que pensez-vous de la qualité des résultats de chaque système ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Réponse :*\n",
    "\n",
    "*Nltk est plus précis que spacy. Il donne aussi plus d'informations sur les entités nommées. Par exemple, il donne le type de l'entité nommée (personne, organisation, etc.) et lie les entités qui le sont. Par exemple ici, \"Mount Everest\" est une seul entité nommée, alors que spacy le découpe en deux entités nommées. Il y a aussi quelques différences sur les type des mots comme avec \"was\" qui est un \"AUX\" pour Spacy et un \"VBD\" pour Nltk.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prise en main des données de test\n",
    "\n",
    "**2a.** Quel est le format du fichier `eng.test.a.conll` ?  Quelle information contient chaque colonne ?  Quel est le format des tags NE ?\n",
    "\n",
    "Note : ce fichier fait partie des données de test pour la NER sur l'anglais de la conférence [CoNLL](https://www.clips.uantwerpen.be/pages/past-workshops) 2003. On peut lire [ici](https://www.clips.uantwerpen.be/conll2003/ner/) la description de la tâche et les scores obtenus.  On peut trouver une copie des données [ici](https://sourceforge.net/p/text-analysis/svn/1243/tree/text-analysis/trunk/Corpora/CoNLL/2003/) ou [ici](https://github.com/synalp/NER/tree/master/corpus/CoNLL-2003).  Les textes proviennent du [corpus Reuters](http://trec.nist.gov/data/reuters/reuters.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first item on each line is a word, the second a part-of-speech (POS) tag, the third a syntactic chunk tag and the fourth the named entity tag.\n",
    "\n",
    "*Réponse :*\n",
    "\n",
    "*Le format du fichier est un fichier texte avec une ligne par mot. Chaque ligne contient 4 colonnes séparées par des espaces. La première colonne contient le mot, la deuxième le tag de la partie du discours (POS), la troisième le tag de la syntaxe et la quatrième le tag de l'entité nommée (NE).*\n",
    "\n",
    "*Le format des tags NE est I-TYPE, ce qui signifie que le mot se trouve à l'intérieur d'une phrase de type TYPE. Ce n'est que si deux phrases du même type se suivent immédiatement que le premier mot de la deuxième phrase aura l'étiquette B-TYPE pour montrer qu'il commence une nouvelle phrase. Un mot portant l'étiquette O ne fait pas partie d'une phrase. TYPE peut être PER (personne), LOC (lieu), ORG (organisation) ou MISC (divers).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2b.** Veuillez charger les données de `eng.test.a.conll` grâce à la classe `ConllCorpusReader` de NLTK vue dans les labos précédents (voir [documentation](https://www.nltk.org/api/nltk.corpus.reader.conll.html#nltk.corpus.reader.conll.ConllCorpusReader)). Veuillez lire les colonnes qui contiennent les tokens ('words'), les POS tags ('pos') et les informations sur les entités nommées ('chunk') et afficher les quatre premières phrases, accessibles via la méthode `.iob_sents()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.conll import ConllCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_corpus = ConllCorpusReader('.', 'eng.test.a.conll', ['words', 'pos', 'ignore', 'chunk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conll Corpus:\n",
      "[('CRICKET', 'NNP', 'O'), ('-', ':', 'O'), ('LEICESTERSHIRE', 'NNP', 'I-ORG'), ('TAKE', 'NNP', 'O'), ('OVER', 'IN', 'O'), ('AT', 'NNP', 'O'), ('TOP', 'NNP', 'O'), ('AFTER', 'NNP', 'O'), ('INNINGS', 'NNP', 'O'), ('VICTORY', 'NN', 'O'), ('.', '.', 'O')]\n",
      "[('LONDON', 'NNP', 'I-LOC'), ('1996-08-30', 'CD', 'O')]\n",
      "[('West', 'NNP', 'I-MISC'), ('Indian', 'NNP', 'I-MISC'), ('all-rounder', 'NN', 'O'), ('Phil', 'NNP', 'I-PER'), ('Simmons', 'NNP', 'I-PER'), ('took', 'VBD', 'O'), ('four', 'CD', 'O'), ('for', 'IN', 'O'), ('38', 'CD', 'O'), ('on', 'IN', 'O'), ('Friday', 'NNP', 'O'), ('as', 'IN', 'O'), ('Leicestershire', 'NNP', 'I-ORG'), ('beat', 'VBD', 'O'), ('Somerset', 'NNP', 'I-ORG'), ('by', 'IN', 'O'), ('an', 'DT', 'O'), ('innings', 'NN', 'O'), ('and', 'CC', 'O'), ('39', 'CD', 'O'), ('runs', 'NNS', 'O'), ('in', 'IN', 'O'), ('two', 'CD', 'O'), ('days', 'NNS', 'O'), ('to', 'TO', 'O'), ('take', 'VB', 'O'), ('over', 'IN', 'O'), ('at', 'IN', 'O'), ('the', 'DT', 'O'), ('head', 'NN', 'O'), ('of', 'IN', 'O'), ('the', 'DT', 'O'), ('county', 'NN', 'O'), ('championship', 'NN', 'O'), ('.', '.', 'O')]\n",
      "[('Their', 'PRP$', 'O'), ('stay', 'NN', 'O'), ('on', 'IN', 'O'), ('top', 'NN', 'O'), (',', ',', 'O'), ('though', 'RB', 'O'), (',', ',', 'O'), ('may', 'MD', 'O'), ('be', 'VB', 'O'), ('short-lived', 'JJ', 'O'), ('as', 'IN', 'O'), ('title', 'NN', 'O'), ('rivals', 'NNS', 'O'), ('Essex', 'NNP', 'I-ORG'), (',', ',', 'O'), ('Derbyshire', 'NNP', 'I-ORG'), ('and', 'CC', 'O'), ('Surrey', 'NNP', 'I-ORG'), ('all', 'DT', 'O'), ('closed', 'VBD', 'O'), ('in', 'RP', 'O'), ('on', 'IN', 'O'), ('victory', 'NN', 'O'), ('while', 'IN', 'O'), ('Kent', 'NNP', 'I-ORG'), ('made', 'VBD', 'O'), ('up', 'RP', 'O'), ('for', 'IN', 'O'), ('lost', 'VBN', 'O'), ('time', 'NN', 'O'), ('in', 'IN', 'O'), ('their', 'PRP$', 'O'), ('rain-affected', 'JJ', 'O'), ('match', 'NN', 'O'), ('against', 'IN', 'O'), ('Nottinghamshire', 'NNP', 'I-ORG'), ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nConll Corpus:\")\n",
    "for sentence in conll_corpus.iob_sents()[1:5]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2c.** Veuillez préparer les données pour le test, en ne gardant que les phrases ayant au moins trois (3) tokens (pas 0, 1, 2) :\n",
    "\n",
    "* une variable `test_tokens` contiendra les tokens groupés par phrase (liste de listes de strings)\n",
    "* une variable `test_tags` contiendra tous les tags NE en une seule liste (en vue de l'évaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tags = []\n",
    "test_tokens = []\n",
    "\n",
    "for sentence in conll_corpus.iob_sents():\n",
    "    tokens_length = len(sentence)\n",
    "    if tokens_length >= 3: # Ignore sentences with less than 3 tokens\n",
    "        test_tokens.append([token for token, _, _ in sentence]) # Extract tokens\n",
    "        for _, _, tag in sentence: # Extract tags\n",
    "            test_tags.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exemple Test Tokens:\n",
      "['CRICKET', '-', 'LEICESTERSHIRE', 'TAKE', 'OVER', 'AT', 'TOP', 'AFTER', 'INNINGS', 'VICTORY', '.']\n",
      "['West', 'Indian', 'all-rounder', 'Phil', 'Simmons', 'took', 'four', 'for', '38', 'on', 'Friday', 'as', 'Leicestershire', 'beat', 'Somerset', 'by', 'an', 'innings', 'and', '39', 'runs', 'in', 'two', 'days', 'to', 'take', 'over', 'at', 'the', 'head', 'of', 'the', 'county', 'championship', '.']\n",
      "['Their', 'stay', 'on', 'top', ',', 'though', ',', 'may', 'be', 'short-lived', 'as', 'title', 'rivals', 'Essex', ',', 'Derbyshire', 'and', 'Surrey', 'all', 'closed', 'in', 'on', 'victory', 'while', 'Kent', 'made', 'up', 'for', 'lost', 'time', 'in', 'their', 'rain-affected', 'match', 'against', 'Nottinghamshire', '.']\n",
      "['After', 'bowling', 'Somerset', 'out', 'for', '83', 'on', 'the', 'opening', 'morning', 'at', 'Grace', 'Road', ',', 'Leicestershire', 'extended', 'their', 'first', 'innings', 'by', '94', 'runs', 'before', 'being', 'bowled', 'out', 'for', '296', 'with', 'England', 'discard', 'Andy', 'Caddick', 'taking', 'three', 'for', '83', '.']\n",
      "['Trailing', 'by', '213', ',', 'Somerset', 'got', 'a', 'solid', 'start', 'to', 'their', 'second', 'innings', 'before', 'Simmons', 'stepped', 'in', 'to', 'bundle', 'them', 'out', 'for', '174', '.']\n",
      "\n",
      "Exemple Test Tags:\n",
      "O\n",
      "O\n",
      "I-ORG\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "O\n",
      "I-MISC\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExemple Test Tokens:\")\n",
    "for tokens in test_tokens[:5]:\n",
    "    print(tokens)\n",
    "\n",
    "print(\"\\nExemple Test Tags:\")\n",
    "for tags in test_tags[:12]:\n",
    "    print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2d.** Combien d'occurrences de tags contient `test_tags`?  Combien de tags différents y a-t-il, et lesquels sont-ils ?  Combien il y a d'occurrences de tags de chaque type ?  Combien de phrases y a-t-il dans `test_tokens` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'occurences de test_tags: 50817\n",
      "Nombre de tags uniques: 5\n",
      "{'O', 'I-LOC', 'I-PER', 'I-ORG', 'I-MISC'}\n",
      "Nombre d'occurences de chaque tag: {'O': 42474, 'I-LOC': 1938, 'I-PER': 3097, 'I-ORG': 2080, 'I-MISC': 1228}\n",
      "Nombre de phrases dans le corpus de test: 2970\n"
     ]
    }
   ],
   "source": [
    "print(\"Nombre d'occurences de test_tags: \" + str(len(test_tags)))\n",
    "print(\"Nombre de tags uniques: \" + str(len(set(test_tags))))\n",
    "print(set(test_tags))\n",
    "print(\"Nombre d'occurences de chaque tag: \" + str({tag: test_tags.count(tag) for tag in set(test_tags)}))\n",
    "\n",
    "print(\"Nombre de phrases dans le corpus de test: \" + str(len(test_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performances de NLTK pour la NER\n",
    "\n",
    "**3a.** Le NER de NLTK a un jeu de tags différents de celui des données de test.  Veuillez chercher les informations pour compléter la fonction suivante qui convertit chaque tag du NER de NLTK vers le tag correspondant pour les données de test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_nltk_conll(nltk_tag):\n",
    "   corresp = {'O':'O',\n",
    "              'ORGANIZATION':'I-ORG',\n",
    "                'PERSON':'I-PER',\n",
    "                'LOCATION':'I-LOC',\n",
    "                'MISC':'I-MISC'} # TODO CONTINUER ICI\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3b.** Veuillez exécuter la NER de NLTK sur chacune des phrases de `test_tokens`, ce qui assure que NLTK aura la même tokenisation que les données de référence.  Veuillez stocker les tags dans une liste unique appelée `nltk_tags`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_tags = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3c.** Veuillez convertir les tags de `nltk_tags` grâce à la fonction précédente, dans une liste appelée `nltk_tags_conv`.  Veuillez afficher le nombre total de tags et les dix premiers.  Vous pouvez plusieurs essais en changeant la fonction, pour aboutir à la conversion qui maximise le score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3d.** Veuillez afficher le rapport d'évaluation de classification obtenu de Scikit-learn et la matrice de confusion pour tous les types de tags apparaissant dans les données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performances de spaCy pour la NER\n",
    "\n",
    "**4a.** Le NER de spaCy a aussi un jeu de tags différents de celui des données de test.  Veuillez chercher les informations pour compléter la fonction suivante qui convertir chaque tag du NER de spaCy dans le tag correspondant pour les données de test.  Attention à la logique des conversions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_spacy_conll(spacy_tag):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4b.** Veuillez exécuter la NER de spaCy sur chacune des phrases de `test_tokens`, ce qui assure que spaCy aura la même tokenisation que les données de référence.  Veuillez stocker les tags dans une liste unique appelée `spacy_tags`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4c.** Veuillez convertir les tags de `spacy_tags` grâce à la fonction précédente, dans une liste appelée `spacy_tags_conv`.  Veuillez afficher le nombre total de tags et les dix premiers.  Vous pouvez plusieurs essais en changeant la fonction, pour aboutir à la conversion qui maximise le score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4d.** Veuillez afficher le rapport d'évaluation de classification obtenu de Scikit-learn et la matrice de confusion pour tous les types de tags apparaissant dans les données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4e.** Veuillez exécuter également le modèle 'en_core_web_lg' de spacy et afficher le rapport d'évaluation (il n'est pas demander d'afficher la matrice de confusion).  Vous pouvez recopier ici le minimum de code nécessaire à l'obtention des résultats, avec une nouvelle pipeline spaCy appelée 'nlp2'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_lg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Utilisation d'embeddings contextualisés fournis par BERT\n",
    "\n",
    "Dans cette section, vous allez évaluer un modèle basé sur le Transformer, qui fournit des embeddings contextualisés pour chaque token.  Ces embeddings sont ensuite utilisés par une couche finale (appelée _NER head_) pour assigner à chaque token une étiquette indiquant les entités nommées.  Le modèle et les instructions pour l'utiliser sont disponibles ici : https://huggingface.co/dslim/distilbert-NER (il s'agit d'une version de BERT \"distillée\" dans un modèle plus léger, suivi du _NER head_).\n",
    "\n",
    "**5a.** Prise en main du modèle : à l'aide des exemples fournis sur Hugging Face, veuillez appliquer DistilBERT_NER sur les 3 premières phrases des données contenues dans `test_tokens` (voir le point 2c) et afficher les résultats obtenus.  Quelles sont les différences avec les résultats de NLTK et de spaCy ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les trois classes, créer le tokenizer, le modèle et la pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester la pipeline sur les 3 premières phrases de test_tokens : afficher la phrase et le résultat de la pipeline.\n",
    "# Concaténer les tokens avec ' '.join(liste_de_tokens) avant de les donner à la pipeline. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5b.** Le modèle DistilBERT utilise son propre tokeniseur, c'est pourquoi les étiquettes indiquant les entités nommées sont parfois portées par plusieurs *subwords* composant un mot.  Il faut donc parfois agréger une liste d'étiquettes en une seule.  \n",
    "\n",
    "Veuillez écrire une fonction qui prend en entrée une liste d'étiquettes et retourne une seule étiquette : soit la première, soit celle qui est majoritaire.  Cette fonction sera utilisée plus loin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 2 (3300681988.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[150], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(tag_fusion(['O', 'B-PER', 'B-PER'], method='majority'))\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after function definition on line 2\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def tag_fusion(tag_list, method='first'): # ou method='majority'\n",
    "\n",
    "print(tag_fusion(['O', 'B-PER', 'B-PER'], method='majority'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5c.** Veuillez afficher le `set` des tags apparaissant dans les résultats de DistilBERT_NER sur les 50 premières phrases du corpus.  Comment se comparent-ils aux tags des données de test CoNLL ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5d.**  Sur le modèle des points 3a et 4a, veuillez écrire une fonction qui convertit les tags générés par DistilBERT_NER aux tags des donnéees de test.  Vous utiliserez cette fonction plus loin.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bert_conll(bert_tag):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5e.** La pipeline DistilBERT_NER utilise son propre tokeniseur, qui ne peut pas être changé, car le modèle DistilBERT a été défini et entraîné avec lui.  On doit donc convertir les tokens du modèle à la même tokenisation que celle des données de test, en vue de l'évaluation.  Pour cela, on vous donne la fonction `convert_tokens` ainsi que du code pour la tester.  Veuillez étudier le code pour pouvoir l'utiliser plus bas, puis répondez aux questions ci-après."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tokens(ref_tokens, bert_result, word_ids):\n",
    "    tokenized = tokenizer.tokenize(' '.join(ref_tokens))\n",
    "    all_tags = []\n",
    "    for tok, i in zip(tokenized, range(len(tokenized))):\n",
    "        label = [tag['entity'] for tag in bert_result if tag['index'] == i+1]\n",
    "        if label:\n",
    "            label = label[0]\n",
    "        else:\n",
    "            label = 'O'\n",
    "        if tok[:2] == '##' or word_ids[i] == word_ids[i-1]: \n",
    "            all_tags[-1].append(label)\n",
    "        else:\n",
    "            all_tags.append([label])\n",
    "    return [tag_fusion(taglist, method='first') for taglist in all_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de la fonction convert_tokens:\n",
    "i = 1 # choix de la phrase\n",
    "def print_len(l):\n",
    "    print(len(l), '--', l)\n",
    "print_len(test_tokens[i]) # Affichage 1\n",
    "tokenized_by_bert = tokenizer(test_tokens[i], add_special_tokens=False, is_split_into_words=True)\n",
    "print_len(tokenizer.convert_ids_to_tokens(tokenized_by_bert[\"input_ids\"]))  # Affichage 2\n",
    "print_len(tokenized_by_bert.word_ids())  # Affichage 3\n",
    "tagged_by_bert = distilbert_ner(' '.join(test_tokens[i]))\n",
    "print_len(tagged_by_bert)  # Affichage 4\n",
    "print_len(convert_tokens(test_tokens[i], tagged_by_bert, tokenized_by_bert.word_ids()))  # Affichage 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions\n",
    "# 1. Que représentent les trois paramètres de convert_tokens ? Définissez-les comme dans une docstring.\n",
    "# 2. Que représentent les cinq lignes affichées par le code de test ?\n",
    "# 3. Quels sont les deux problèmes traités par la fonction convert_tokens ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5f.** Veuillez effectuer la reconnaissance des entités nommées avec la pipeline DistilBERT_NER et obtenir la liste finale de tags avec les noms convertis à ceux du jeu de référence grâce à `convert_bert_conll`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 20 # se limiter aux j premières phrases sur les 2970 (utile pendant le développement, mais les utiliser toutes à la fin)\n",
    "bert_tags_conv = []\n",
    "\n",
    "print(len(bert_tags_conv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5g.** En comparant `test_tags` avec `bert_tags_conv`, veuillez afficher le rapport d'évaluation de la classification et la matrice de confusion, pour tous les types de tags apparaissant dans les données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5h.** Laquelle des deux stratégies de `tag_fusion` (premier tag ou tag majoritaire) conduit à de meilleurs résultats ?  Veuillez effectuer l'expérience et indiquer simplement les scores obtenus et votre conclusion dans le champ suivant.  Pour la conclusion finale, gardez 'first'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Discussion finale\n",
    "\n",
    "Veuillez comparer les scores des quatre modèles testés, en termes de **macro avg**.  Pourquoi ce score est-il le plus informatif ?  Veuillez indiquer également la taille des modèles spaCy évalués."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fin du Labo.** Veuillez nettoyer ce notebook en gardant seulement les résultats désirés, l'enregistrer, et le soumettre comme devoir sur Cyberlearn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
